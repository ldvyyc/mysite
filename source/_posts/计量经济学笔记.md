
---
title: è®¡é‡ç»æµŽå­¦ç¬”è®°
author: Frank Yu
categories: Learning
img: https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230410114258.png
toc: true
mathjax: true
cover: false
top: false
summary: çŽ‹æ™¨æºè€å¸ˆè¯¾ç¨‹ç¬”è®°
date: 2023-04-10, Monday
time: 11:41
tags: 
- Finance
- Notes
---


## Ch1 Introductory

### what is econometrics

* Combine statistical techniques with economic theory. 
* Estimating economic relationships.
* testing economic theories.
* evaluating and implementing government and business policy.

### basic types

#### Descriptive

* challenges
	* Sampling
		* draw conclosion about the population based on sample.
	* Summary statistics
		* nice way to summarize complicated data.
* If we have data we would know the answer.
* Conditional Expectations: if I condition X to be some value, what is the expect value of Y. Often a variable that can take on a very large number of values is treated as continuous for convenient.
* Example
	* *Mothers smoke one more cigarette during pregnancy are expected to give birth to child with 15g lower birth weight.*

#### Forecasting

* challenges
	* Underfitting
	* Overfitting
* If we know the data and wait long enough, we will know the answer.

#### Causal ï¼ˆfor structuralï¼‰

* Correlation: how two random variables move together 
* The difference between causation and correlation is a key concept in econometrics. We would like to identify causal effects and estimate their magnitude. 
* It is generally agreed that this is very difficult to do; having an economic model is often essential in establishing the causal interpretation.
* **Unless run perfect experiment, we will never know the answer.**
* requires $ð¸(ð‘¢|ð‘¥) = 0$
* **Econometrics focus on causal problems inherent in collecting and analyzing observational economic data.** 
* æœ‰å‡ ç§å¯èƒ½ï¼š
	* x -> y
	* z -> x, z -> y
	* y -> x
* Example
	* *Mothers smoke one more cigarette during pregnancy causes their child to have 15g lower birth weight.*

### Structure of Economic data

#### Cross-sectional data

* A cross-sectional data set consists of a sample of units taken at a given point in time. 
* æˆªé¢æ•°æ®
* assume:
	* sample is drawn from the underlying population randomly
	* Violation of random sampling: We want to obtain a random sample of family income. However, wealthier families are more likely to refuse to report.

#### Time-Series data

* Each observation is uniquely determined by time 
* æ—¶é—´åºåˆ—
* A time series data set consists of observations on a variable or several variables over time.
	* å¯ä»¥æ˜¯å¯¹å¤šä¸ªå˜é‡çš„æ—¶é—´åºåˆ—
* Time is an important dimension in a time series data set.

#### Pooled Cross Sections and Panel or Longitudinal Data

* Pooled cross sections include cross-sectional data in multiple years. 
* A panel data set consists of a time series for each cross-sectional member in the data set.
* Panel data: 
	* the same units over time
* pooled cross sections: 
	* diferent units, diferent time.
* Each observation is uniquely determined by the unit and the time.
* åŒæ—¶å…·å¤‡æ—¶é—´å’Œå˜é‡å·®å¼‚

---

## Ch2 The Simple Regression Model: 

### Interpretation and Estimation

#### Descriptive analysis

* Define conditional expectation Eï¼ˆy|xï¼‰
	* if I condition X to be some value, what is the expected value of Y?

#### Simple Linear model

* $$E(y|x)=\beta_0+\beta_1x$$
* $$\beta_0=E(y|x=0)$$
* $$\beta_1=\frac{\partial E(y|x)}{\partial x}$$
* let $u=y-E(y|x)$, thus $E(u|x)=0$
* $$\hat{u_i}=y_i-\hat{\beta_0}-\hat{\beta_1}x_i$$
* using law of iterated expectation, we get
	* $E(u)=0$, and $E(ux)=0$

* å¯¹äºŽæ€»ä½“ï¼Œå†™æˆ $y_i=\beta_0+\beta_1x_i+u_i$
* å¯¹äºŽæ ·æœ¬ï¼Œå†™æˆ $y_i=\hat{\beta_0}+\hat{\beta_1}x_i+\hat{u_i}$
	* å¸¦å¸½å­è¡¨ç¤ºçš„æ˜¯æ ·æœ¬ï¼Œç”¨æ¥ä¼°è®¡å®žé™…å€¼

##### Method of Moments

* Method of moments: use the sample average to estimate the population expectation 
* Use $\frac{1}{N}\sum$ to replace $E[Â·]$
| population expectations | sample analogue                  |
| ----------------------- | -------------------------------- |
| $E(u)=0$                | $\frac{1}{N}\sum \hat{u_i}=0$    |
| $E(ux)=0$               | $\frac{1}{N}\sum x_i\hat{u_i}=0$ |

* using $\hat{u_i}=y_i-\hat{\beta_0}-\hat{\beta_1}x_i$ to represent u, the result is :
	* $$\hat{\beta_1}=\frac{\frac{1}{N}\sum_{i=1}^N(x_i-\bar{x})(y_i-\bar{y})}{\frac{1}{N}\sum_{i=1}^N(x_i-\bar{x})^2}$$
	* $$\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$$


#### Causal Estimation

* $$y=\beta_0+\beta_1 x+u$$
	* $\beta_{0}$ and $\beta_1$ are unknown numbers in the nature we want to uncover
	* You choose x
	* Nature chose u in a way that is unrelated to your choice of x
* u represent things affect y other than x
	* we think of u as some real thing, It's just we can't observe it
	* u æ˜¯ä¸€ä¸ªå®žé™…å­˜åœ¨çš„å˜é‡
	* To estimate the model,we need to know how u is determined.
	* The simplest case is that u is assigned at random 
	* We can write this as $E(u|x)=0$.
		* **u ä¸éšç€ x è€Œå˜**
			* é‡è¦ï¼åæ–¹å·®ä¸º0
		* å‡å€¼æ˜¯0
* å¯¹äºŽä¸Šå¼ä¸¤è¾¹å–æœŸæœ›ï¼Œå¯¹xæ±‚åå¯¼ï¼š
	* $$\frac{\partial E[y|x]}{\partial x}=\beta_{1}+\frac{\partial E[u|x]}{\partial x}$$
	* åªæœ‰å½“$E[u|x]$ æ˜¯å¸¸æ•°æ—¶,$\beta_1$ è¡¨ç¤ºå½“xå˜åŒ–1æ—¶ï¼Œyçš„å¹³å‡å˜åŒ–ã€‚
	* This condition gives the model a causal interpretation:
		* if $E(u|x)$ does not vary when x changes, then any change in y can be attributed to x
	* å› æ­¤$\beta_{1}$ ååº”äº†xå¯¹yçš„å› æžœå…³ç³»

#### Forecasting

* æˆ‘ä»¬å¸Œæœ›å¾—åˆ°ï¼š
	* $$\hat{y}^*=\hat{\beta_0}+\hat{\beta_1}x^*$$
* å’Œcausalä¸åŒï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰è‡ªå·±é€‰æ‹© $x^*$
* ä½¿ç”¨æœ€å°äºŒä¹˜æ³•ï¼Œargmaxæ¥åˆ¤æ–­å›žå½’çš„yå’ŒçœŸå®žå€¼çš„å…³ç³»ã€‚


**ä¸¤ç§æ–¹æ³•ï¼šmomentå’ŒOLS**


### Properties of Simple Regression Model

#### Properties of OLS on Any Sample of Data


* $$\sum_{i=1}^{N}\hat{u}_{i}=0.$$


* $$\sum_{i=1}^{N}x_{i}{\hat{u}}_{i}=0.$$


* $$\sum_{i=1}^N\hat{y}_i\hat{u}_i=0$$


* $$\hat{\beta_0}+\hat{\beta_1}\bar{x}=\bar{y}$$

#### Goodness of Fit

* measure how well our model fits the data
* decompose $y_i$ into two parts: the fitted value and the residual.
	* $$y_i=\hat{y_i}+\hat{u_i}$$
	* ç¬¬ä¸€éƒ¨åˆ†æ˜¯æ¨¡åž‹è§£é‡Šçš„ï¼Œç¬¬äºŒéƒ¨åˆ†ä¸æ˜¯
* Define the following termsï¼š
	* Total sum of squaresï¼ˆSSTï¼‰$$S S T=\sum_{i=1}^{N}\bigl(y_{i}-\bar{y}\bigr)^{2}.$$

	* Explained sum of squaresï¼ˆSSEï¼‰$$S S E=\sum_{i=1}^{N}({\hat{y}}_{i}-{\bar{y}})^{2}.$$
	* Residual sum of squaresï¼ˆSSRï¼‰$$S S R=\sum_{i=1}^{N}\hat{u}_{i}^{2}.$$
	* $$SST=SSE+SSR$$
* å®šä¹‰ Goodness of fitï¼Œ $R^2$
	* $$R^2=\frac{SSE}{SST}$$
	* å³yæœ‰å¤šå¤§çš„éƒ¨åˆ†æ˜¯ç”±$y_i$ è§£é‡Šçš„
	* æ€»æ˜¯åœ¨0åˆ°1ä¹‹é—´
	* åªæ˜¯æè¿°äº†xå’Œyçš„ç›¸å…³æ€§ï¼Œå¹¶ä¸èƒ½è¡¨ç¤ºå› æžœå…³ç³»


#### Functional form

* $$log(y)=\beta_0+\beta_1 x+u$$
	* $$\beta_1=\frac{dlog(y)}{dy}=\frac{dy}{y}\cdot\frac{1}{x}$$


* ![Functional forms involving log](https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230407231705.png)

#### Expected Values and Variances of the OLS Estimators

##### Unbiasedness of OLS

* Unbiasedness means the expectation of the estimator equals the true value.
* æ— åæ€§
* $E(\hat{\beta})=\beta$

##### å‡è®¾ï¼šSLRï¼ˆsimple linear regressionï¼‰

1. Linear in Parameters 
2. Random Sampling 
	* $Cov(u_i,u_j)=0$ 
3. Sample Variation in the Explanatory Variable 
	* å³è‡ªå˜é‡xçš„å–å€¼ä¸èƒ½åªæœ‰ä¸€ä¸ªç‚¹ 
4. Zero Conditional Mean: $E(u|x) = 0$

åœ¨ä¸Šé¢å››ä¸ªæ¡ä»¶æˆç«‹æ—¶ï¼Œ$\beta_0$ å’Œ $\beta_1$ éƒ½æ»¡è¶³æ— åæ€§

* Though the OLS estimator is unbiased, it is still possible that the estimates calculated using the sample is very different from Î² in the population.

5. Homoskedasticity 
	* The error u has the same variance given any value of the explanatory variable. 
	* In other words, $Var(u|x)=\sigma^2$
	* åŒæ–¹å·®æ€§
	* ![left: homoskedasticity; right: heteroskedasticity](https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230408134326.png)

åœ¨ä¸Šé¢5ä¸ªæ¡ä»¶æˆç«‹æ—¶ï¼Œå¯ä»¥æ±‚æ–¹å·®ï¼š
* $$Var(\hat{\beta_1}|x)=\frac{\sigma^2}{\sum_{i=1}^N(x_i-\bar{x})^2}=\frac{\sigma^2}{SST_x}$$
	* å…¶ä¸­ $\sigma$ æ˜¯ u çš„æ ‡å‡†å·®
	* å½“ $\sigma$ å¤§æ—¶$V a r(\hat{\beta}_{1}|x)$ æ–¹å·®å¤§
	* å½“ x çš„æ–¹å·®å¤§æ—¶ $V a r(\hat{\beta}_{1}|x)$ å°
* $$V a r(\hat{\beta_0}|x)=\frac{\sigma^2\sum_{i=1}^Nx_i^2}{N\sum_{i=1}^N(x_i-\bar{x})^2}$$

##### ä¼°è®¡$\sigma$

* éœ€è¦ç”¨sampleæ¥ä¼°è®¡ $\sigma^2$
* $$s^2\equiv\hat{\sigma}^2=\frac{1}{N-2}\sum_{i=1}^N\hat{u}_i^2$$
* è¿™æ˜¯æ— åä¼°è®¡ï¼Œå…¶ä¸­-2æ˜¯å› ä¸ºæœ‰ä¸¤ä¸ªçº¦æŸæ¡ä»¶ï¼Œå°‘äº†ä¸¤ä¸ªè‡ªç”±åº¦ã€‚


## Ch3 Multiple Regression Analysis: Estimation

### Why we need multiple regression model?

* Descriptive analysis: sometimes we want to estimate the conditional mean of y on multiple variables
* Causal estimation: we know that something other than x may afect y, so we explicitly control them.
* Forecasting: we want to use more variables to better predict y

### Estimation and Interpretation

* Population regression model:$$y=\beta_0+\beta_1 x_1+\cdots+\beta_nx_n+u$$
	* zero conditional mean:
		* $$E(u|x_1,\cdots,x_n)=0$$
	* besides, using law of iterated expectation
		* $$E(x_ju)=0\quad E(u)=0$$
* Fitted value: $$\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_{i1}+\cdots+\hat{\beta_k}x_{i k}$$
* residual: $$\hat{u_i}= y_{i}-\hat{y_i}$$

#### Sample analog

| population expectations | sample analogue                     |
| ----------------------- | ----------------------------------- |
| $E(u)=0$                | $\frac{1}{N}\sum \hat{u_i}=0$       |
| $E(x_1u)=0$             | $\frac{1}{N}\sum x_{i1}\hat{u_i}=0$ |
| $E(x_ku)=0$             | $\frac{1}{N}\sum x_{ik}\hat{u_i}=0$ |

#### OLS

* $$H\equiv\sum_{i=1}^n\hat{u_i}^2=\sum(y_i-b_0-b_1x_{i1}-\cdots-b_kx_{i k})^2$$
* æ±‚æœ€å°å€¼ï¼Œå¾—ä¸€é˜¶æ¡ä»¶ï¼š
	* $$\frac{\partial H}{\partial b_0}=-\sum_{i=1}^n2(y_i-\hat{\beta_0}-\hat{\beta_1}x_{i1}-\cdots-\hat{\beta_k}x_{ik})=0$$
	* $${\frac{\partial H}{\partial b_{j}}}=-\sum_{i=1}^{n}2x_{i j}(y_{i}-{\hat{\beta_0}}-{\hat{\beta_1}}x_{i1}-\cdots-{\hat{\beta_k}}x_{i k})=0,\forall j=1,2,...,k.$$
* OLS and sample analogue give the same answer.


#### Interpretation

* The coeicient of $x_i$ represents holding fixed other factors, the change in y when $x_i$ increases by one unit.
* $$\Delta\hat{y}=\hat{\beta}_1 \Delta x_1+\hat{\beta}_2 \Delta x_2+\cdots+\hat{\beta}_k \Delta x_k$$

* æ±‚ $\hat{\beta_j}$: Frisch-Waugh-Lovell Theorem
	* Regress $x_j$ on other independent variables ï¼ˆincluding the constantï¼‰, obtain the residual $\hat{r_{ij}}$.
	* Regress y on other independent variables ï¼ˆincluding the constantï¼‰, obtain the residual $\hat{r_{iy}}$.
	* Regress $\hat{r_{iy}}$ on $\hat{r_{ij}}$, The resulting slope coefficient is $\hat{\beta_j}$

#### Goodness of fit

* å’Œ SLR ç›¸åŒ
* ä½†éšç€å˜é‡å¢žåŠ  $R^2$ å‡ ä¹Žä¸€å®šä¼šå¢žå¤§
* å¼•å…¥ Adjusted R2
	* $$\bar{R}^2=1-\frac{SSR/(N-k-1)}{SST/N-1}=1-(1-R^2)\frac{N-1}{N-k-1}$$
	* N represents the size of the sample, k represents the number of independent variables ï¼ˆexcluding the constantï¼‰



### Expected Values and Variances of the OLS Estimators

#### å‡è®¾ï¼šMLRï¼ˆmultiple linear regressionï¼‰

1. Linear in Parameters 
2. Random Sampling 
	* $Cov(u_i,u_j)=0$ 
3. No perfect collinearity
	* ä¸èƒ½çº¿æ€§ç›¸å…³
	* ä¸ç„¶æ— æ³•åŒºåˆ«è¿™äº›çº¿æ€§ç›¸å…³æˆåˆ†
4. Zero Conditional Mean: 
	* $E(u|x_1,\cdots,x_k) = 0$

* åœ¨ä¸Šé¢å››ä¸ªæ¡ä»¶æˆç«‹æ—¶ï¼Œ$\beta_0$ å’Œ $\beta_1$ éƒ½æ»¡è¶³æ— åæ€§

5. Homoskedasticity 
	* $Var(u|x_1,\cdots,x_k)=\sigma^2$
	* åŒæ–¹å·®æ€§

* MLR1-5åˆæˆ Gauss-Markov Assumption
* åœ¨Gauss-Markovæ¡ä»¶ä¸‹ï¼Œæ–¹å·®æ»¡è¶³ï¼š
	* $$V a r(\hat{\beta_j})=\frac{\sigma^{2}}{SST_j(1-R_j^2)}$$
	* $R_j^2$ is the R-squared from regressing $x_j$ on all other independent variables and including an intercept

* The unbiased estimator of $Ïƒ^2$ is:
	* $$\hat{\sigma}^{2}=\frac{1}{N-k-1}\sum_{i=1}^{n}\hat{u}_{i}^{2}.$$
	* è‡ªç”±åº¦ï¼šN-k-1
	* æ˜¯æ— åä¼°è®¡

#### BLUE

| sdandard deviation                      | standard error                                   |
| --------------------------------------- | ------------------------------------------------ |
| $sd(\hat{\beta_j})$                     | $se(\hat{\beta_j})$                              |
| $\frac{\sigma}{[SST_j(1-R_j^2)]^{1/2}}$ | $\frac{\hat{\sigma}}{[SST_j(1-R_j^2)]^{1/2}}$    |
| $\sigma^2=\text{Var of u}$              | $\hat{\sigma}^{2}=\frac{1}{n-k-1}\sum \hat{u}^2$ |
| unknown                                        |  estimated using sample                                                |

* OLS æ˜¯ best linear unbiased estimator, BLUE
	* æ»¡è¶³çº¿æ€§ï¼Œå¹¶ä¸”æ–¹å·®æœ€å°ã€‚


### Practical issues

#### Omitted bias

* å‡è®¾æœ‰ä¸¤ä¸ªè‡ªå˜é‡ï¼Œä½†åªå¯¹å…¶ä¸­ä¸€ä¸ªè¿›è¡Œå›žå½’ï¼Œé‚£ä¹ˆå¾—åˆ°çš„$\hat{\beta}$ ä¸Žå®žé™…å€¼æœ‰ä¸€ä¸ªè¯¯å·®ã€‚
* $E(\tilde{\beta}_1)=\beta_1+\beta_2\tilde{\delta}_1$
* thus, $Bias(\tilde{\beta_1})=\beta_{2}\tilde{\delta}_{1}$
* |     | $Corr(x_1,x_2)>0$   | $Corr(x_1,x_2)<0$   |
| --- | ------------------- | ------------------- |
| $\beta_2>0$    | Positive Bias | Negative Bias |
| $\beta_2<0$    | Negative Bias | Positive Bias |

* å½±å“æ— åæ€§

#### including irrelavent

* ä¸å½±å“æ— åæ€§ï¼Œä½†æ–¹å·®ä¼šå˜å¤§

#### Multicollinearity

* high ï¼ˆbut not perfectï¼‰ correlation between two or more independent variables
* ä¸å½±å“æ— åæ€§ï¼Œä½†æ–¹å·®ä¼šå˜å¤§

## Ch4 Multiple Regression Analysis: Inference

### Classical Linear Regression Model

#### The Distribution of $\hat{Î²_j}$

* ä¸ºäº†å¾—åˆ°åˆ†å¸ƒï¼Œåœ¨Gauss-Markovæ¡ä»¶ä¸Šè¿˜è¦åŠ ä¸Šä¸€æ¡ï¼š
* MLR6: Normality
	* $$u\sim N(0,\sigma^2)$$

* Assumptions MLR.1 through MLR.6 are called the classical linear model **CLM** assumptions. 
* summarize the population assumptions of the CLM is
	* $$y|{\bf x}\ \sim\ No r m a l(\beta_{0}\ +\beta_{1}x_{1}+\ldots+\beta_{k}x_{k},\sigma^{2})$$

* åœ¨CLMæ¡ä»¶ä¸‹ï¼Œ$\beta$ æœä»Žæ­£æ€åˆ†å¸ƒ
	* $$\frac{\hat{\beta}_j-\beta_j}{sd(\hat{\beta}_j)}\sim Normal(0,1)$$
* ä½†ç”±äºŽå®žé™…ä¸­æ ‡å‡†å·®ä¸çŸ¥é“ï¼Œéœ€è¦ç”¨æ ‡å‡†è¯¯æ¥ç®—ã€‚
	* $$\frac{\hat{\beta_j}-\beta_j}{s e(\hat{\beta_j})}\sim t_{N-k-1}= t_{d f},$$
	* æœä»Žtåˆ†å¸ƒï¼ŒN-k-1æ˜¯è‡ªç”±åº¦ï¼ŒNæ˜¯å¤šå°‘ä¸ªæ ·æœ¬ï¼Œkæ˜¯å›žå½’é‡Œé¢æœ‰å¤šå°‘ä¸ªx


### tæ£€éªŒ

* æ£€éªŒæŸä¸ªç³»æ•°æ˜¯å¦ä¸º0

#### Null hypothesis

* Let $H_0$ be the null hypothesis that we want to test. Let $H_1$ be the alternative hypothesis. 
* We reject the null hypothesis when the test statistic falls in the rejection region.

#### rejection region

* type 1 error:
	* significance level = Î± = $Pr(\text{rejecting }H_0|H_0\text{ is true})$
* type 2 error:
	* $Pr(\text{not rejecting }H_0|H_1\text{ is true})$
* æˆ‘ä»¬çš„æƒ³æ³•æ˜¯å…ˆå›ºå®šä¸€ä¸ªsignificance levelï¼Œç¡®å®šå¯¹type 1 error çš„å®¹å¿åº¦ï¼Œç„¶åŽå†æœ€å°åŒ– type 2 error


#### Testing Against One-Sided Alternatives
* Suppose we are interested in testing
$$
\begin{array}{ll}
H_0: & \beta_j=0 . \\\\
H_1: & \beta_j>0 .
\end{array}
$$
* Consider a test statistic:$$\frac{\hat{\beta}_j-\beta_j}{\operatorname{se(}(\hat{\beta}_j)}$$
* When $H_0$ is true, the test statistic is: $$\frac{\hat{\beta_j}}{s e(\hat{\beta_j})} \sim t_{N-k-1}=t_{df}$$
1. It depends on the data.
2. We know its distribution under $H_0$.

* ä»¤ $$t_{\hat{\beta}_j} \equiv \frac{\hat{\beta}_j}{s e(\hat{\beta}_j)}$$
* We often call $t_{\hat{\beta}_j}$ t-statistic or t-ratio of $\hat{\beta}_j$.
* $t_{\hat{\beta}_j}$ has the same sign as $\hat{\beta}_j$, because $\operatorname{se}(\hat{\beta}_j)>0$.
- ç›´è§‰ä¸Š, å½“ $t_\hat{\beta_j}$ è¶³å¤Ÿå¤§çš„æ—¶å€™æ‹’ç» $H_0$ :  $t_\hat{\beta_i}$ è¶Šå¤§,  $H_0$ æ˜¯çœŸçš„å¯èƒ½æ€§è¶Šä½Ž,  $H_1$ æ˜¯çœŸçš„å¯èƒ½æ€§è¶Šé«˜ã€‚
* å¤šå¤§ç®—è¶³å¤Ÿå¤§ï¼Ÿ
	* Fix a significance level of $5 \%$. The critical value, $c$ is the 95 th percentile when $H_0$ is true. It means when $H_0$ is true, the probability of getting a value as large as $c$ is $5 \%$.
	* Rejection rule:$t_{\hat{\beta}_j}>c$
* Rejecting $H_0$ when $t_{\hat{\beta}_j}>c$ means the probability of making a type I error, that is, the probability of rejecting $H_0$ when $H_0$ is true, is $5 \%$.
* ![å•è¾¹æ£€éªŒï¼Œ5%](https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230408165619.png)

##### The idea of test
1. Fix a significance level $\alpha$. That is, decide our level of "tolerence" for the type I error.
2. Find the critical value associated with $\alpha$. For $H_1: \beta_j>0$, this means finding the $(1-\alpha)$-th percentile of the $\mathrm{t}$ distribution with $d f=N-k-1$.
3. Reject $H_0$ if  $t_{\hat{\beta}_j}>c$

* é€šå¸¸ç¬¬ä¸€ç±»é”™è¯¯å’Œç¬¬äºŒç±»é”™è¯¯æ˜¯ä¸èƒ½åŒæ—¶ç¼©å°çš„ã€‚éœ€è¦å–èˆã€‚


#### Two-sided Alternatives
* We want to test:$$\begin{array}{ll}H_0: & \beta_j=0 . \\\\ H_1: & \beta_j \neq 0 .
\end{array}$$
* This is the relevant alternative when the sign of $\beta_j$ is not well determined by theory.
* Even when we know whether $\beta_j$ is positive or negative under the alternative, a two-sided test is often prudent.
* æ±‚æ³•ï¼š
	1. Fix a significance level $\alpha$. That is, decide our level of "tolerence" for the type I error.
	2. Find the critical value associated with $\alpha$. For $H_1: \beta_j \neq 0$, this means finding the $(1-\alpha / 2)$-th percentile of the $\mathrm{t}$ distribution with $d f=N-k-1$.
	3. Reject $H_0$ if$$|t_{\hat{\beta}_j}|>c .$$
* ![åŒè¾¹æ£€éªŒï¼Œ5%](https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230408165324.png)

* æ²¡æœ‰è¯´æ˜Žçš„è¯é€šå¸¸æ˜¯åŒè¾¹çš„
* If $H_0$ is rejected in favor of $H_1: \quad \beta_j \neq 0$ at the $5 \%$ level, we usually say that " $x_j$ is statistically significant, or statistically different from zero, at the $5 \%$ level."
* If $H_0$ is not rejected, we say that " $x_j$ is statistically insignificant at the $5 \%$ level."

#### Other Hypothesis
* If the null is stated as:
$$
H_0: \beta_j=a_j
$$
Then the t-statistic is$$\frac{\hat{\beta_j}-a_j}{\operatorname{se}(\hat{\beta_j})} \sim t_{N-k-1}$$
We can use the general t statistic to test against one-sided or two-sided alternatives.

#### p-Values for t Tests

* Given the observed value of the t statistic, what is the smallest significance level at which the null hypothesis would be rejected? 
* We call this â€œsmallest signiicance levelâ€ p-value. 
* p-value represents the probability of observing a value as extreme as $t_{\hat{\beta}_j}$ under the $H_0$
* $$
\begin{array}{ll}
H_0: & \beta_j=0 . \\\\
H_1: & \beta_j \neq 0 .
\end{array}
$$
	* The p-value in this case is$$P(|T|>|t|),$$
	* where we let $T$ denote a $\mathrm{t}$ distributed random variable with $N-k-1$ degrees of freedom and let $t$ denote the numerical value of the test statistic.
- The p-value nicely summarizes the strength or weakness of the empirical evidence against the null hypothesis.
- The p-value is the probability of observing a $t$ statistic as extreme as we did if the null hypothesis is true.
- Signiicance level and critical valueä¸€ä¸€å¯¹åº”

#### Economic versus Statistical Signiicance

* The statistical significance of a variable $x_j$ is determined entirely by the size of $t_{\hat{\beta}_j}$, whereas the economic significance or practical significance of a variable is related to the size (and sign) of $\hat{\beta}_j$.
- We often care about both statistical significance and economic significance.

### Confidence interval

* We can construct a confidence level depending on $\alpha$. We call it a $(1-\alpha)$ confidence interval:$$[\hat{\beta}_j-c \cdot \operatorname{se}(\hat{\beta}_j), \hat{\beta}_j+c \cdot \operatorname{se}(\hat{\beta}_j)]$$
* The critical value $c$ is the $(1-\alpha / 2)$ percentile in a $\mathrm{t}$ distribution with $d f=N-k-1$.
* The meaning of a 95% conidence interval: if we sample repeatedly many times, then the true $Î²_j$ will appear in 95% of the confidence intervals. 
* å¯¹äºŽå¾ˆå¤šæ¬¡å–æ ·æ¥è¯´çš„å¯èƒ½æ€§ï¼Œä½†å¯¹äºŽæŸä¸€æ¬¡ç‰¹å®šçš„å–æ ·ä¸èƒ½ç¡®å®šæ˜¯å¦ä¸€å®šåœ¨ç½®ä¿¡åŒºé—´é‡Œé¢ã€‚

**ä¸‰ç§æ–¹æ³•ä¸€æ ·ï¼š**
1. Fix a significance level $\alpha$, calculate the critical value $c$, and then reject $H_0$ if $|t_{\hat{\beta}_j}|>c$.
2. Fix a significance level $\alpha$, calculate the $\mathrm{p}$-value, reject $H_0$ if $p<\alpha$.
3. reject if 0 is not in the confidence level.

### Testing Multiple Linear Restrictions: The F Test

* $$y=\beta_0+\beta_1 x_1+\beta_2 x_2+\beta_3 x_3+u$$We want to test$$H_0: \quad \beta_1=0 \text { and } \beta_2=0$$$$H_1: H_0\text{ is not true.}$$ 
* Method:
	- Consider the restricted model when $H_0$ is true$$y=\gamma_0+\gamma_3 x_3+u$$
	- If $H_0$ is true, the two models are the same. That means when we include $x_1$ and $x_2$ into the model, the sum of squared residuals should not change much.
	- However, if $H_0$ is false that means that at least one of $\beta_1, \beta_2$ is nonzero and the sum of squared residuals should fall when we include these new variables
	- çœ‹SSRæ˜¯å¦ç›¸ç­‰
- F-test
	- $$F \equiv \frac{(S S R_r-S S R_{u r}) / q}{S S R_{u r} /(N-k-1)}= \frac{(R_{u r}^2-R_r^2) / q}{(1-R_{u r}^2) /(N-k-1)}$$
	- $q$ is the number of linear restrictions, which is the difference in degrees of freedom in the restricted model versus the unrestricted model.
	- $S S R_r$ is the sum of squared residuals from the restricted model and $S S R_{u r}$ is the sum of squared residuals from the unrestricted model.
	- Since $S S R_r$ can be no smaller than $S S R_{u r}$, the $\mathrm{F}$ statistic is always nonnegative.
	- We can show that the sampling distribution of the F-stat: $F \sim F_{q, N-k-1}$. We call this an $\mathrm{F}$ distribution with $q$ degrees of freedom in the numerator and $N-k-1$ degrees of freedom in the denominator.
	- ![F-test 5%](https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230408191204.png)
- tåˆ†å¸ƒçš„å¹³æ–¹å°±æ˜¯Fåˆ†å¸ƒï¼Œå› æ­¤å•å˜é‡ä¹Ÿå¯ä»¥ç”¨Fåˆ†å¸ƒã€‚
- In the $\mathrm{F}$ testing context, the $\mathrm{p}$-value is defined as$$P(\mathcal{F}>F) $$where $\mathcal{F}$ denote an $\mathrm{F}$ random variable with $(q, N-k-1)$ degrees of freedom, and $\mathrm{F}$ is the actual value of the test statistic.
- p-value is the probability of observing a value of $\mathrm{F}$ at least as large as we did, given that the null hypothesis is true.
	- $\rightarrow$ Reject $H_0$ if $p<\alpha$

* é€šå¸¸æƒ…å†µä¸‹ä½¿ç”¨SSRå½¢å¼çš„Fæ£€éªŒæ¯”è¾ƒå¥½ï¼Œå› ä¸ºæœ‰æ—¶å€™restrictedçš„å½¢å¼ä¸Žunrestrictedä¸åŒï¼Œä¸èƒ½ç›´æŽ¥ç”¨R2æ¥è®¡ç®—ã€‚

### Testing Multiple Linear Restrictions: The LM statistic

* Lagrange multiplier ï¼ˆLMï¼‰ statistic
* The LM statistic can be used in testing multiple exclusion restrictions ï¼ˆas in an $F$ testï¼‰ under large sample.
* å¯¹äºŽæ¨¡åž‹$$y=\beta_0+\beta_1 x_1+\ldots+\beta_k x_k+u$$We want to test whether the last $q$ of these variables all have zero population parameters:$$H_0: \beta_{k-q+1}=\beta_{k-q+2}=\ldots=\beta_k=0$$
- $L M$ statistic
- First estimate the restricted model:$$y=\tilde{\beta_0}+\tilde{\beta_1} x_1+\cdots+\tilde{\beta_{k-q}} x_{k-q}+\tilde{u}$$If the coefficients of the excluded independent variables $x_{k-q+1}$ to $x_k$ are truly zero in the population model, then they should be uncorrelated to $\tilde{u}$.
- So regress $\tilde{u}$ on all $x$$$\tilde{u} \sim x_1, x_2, \ldots, x_k$$Let $R_u^2$ denote the R-squared of this regression. The smaller the $R_u^2$, the more likely $H_0$ is true. So a large $R_u^2$ provides evidence against $H_0$.
- $L M=N \cdot R_u^2$. We can show that $L M$ follows chi-square distribution with $q$ degrees of freedom: $\mathcal{X}_q^2$.
	- Reject $H_0$ if $L M>$ critical value $(p<$ significance level)
	- ![chi-square](https://cdn.mathpix.com/snip/images/X3leSxpzRqmmLAR3CZWjQs6eNsBj0o1ySEyLiIXgqNI.original.fullsize.png)
- åœ¨å¤§æ ·æœ¬æ—¶ï¼ŒLMå’ŒFæ£€éªŒç»“æžœå¾ˆç›¸ä¼¼




## Ch5 Multiple Regression Analysis: OLS Asymptotics

### Asymptotic Properties

* Finite sample properties: properties hold for any sample of data.
* Examples
	* Unbiasedness of OLS
	* OLS is BLUE
	* Sampling distribution of the OLS estimators
* Asymptotic properties or large sample properties: not defined for a particular sample size; rather, they are defined as the sample size grows without bound.
	* æ¸è¿‘æ€§è´¨

### Consistency

* ä¸€è‡´æ€§
* Let $W_N$ be an estimator of $\theta$ based on a sample $Y_1, Y_2, \ldots, Y_N$ of size $N$. Then, $W_N$ is a consistent estimator of $\theta$ if for every $\epsilon>0$$$P(|W_N-\theta|>\epsilon) \rightarrow 0 \text { as } N \rightarrow \infty .$$
* We also say consistency means:$$\operatorname{plim}(W_N)=\theta$$
	* Intuitively, consistency means when the sample size becomes larger, the estimator gets closer and closer to the true value.
* **ä¸€è‡´æ€§å’Œæ— åæ€§æ²¡æœ‰å¿…ç„¶è”ç³»**

#### Consistency of OLS
* Under Assumptions MLR.1 through MLR.4, the OLS estimator $\hat{\beta}_j$ is consistent for $\beta_j$, for all $j=0,1, \ldots, k$.
- When the sample size is larger, the OLS estimator is centered around the true parameter closer and closer.

#### Central Limit Theorem
* Use the notation$$\hat{\theta}_N \stackrel{a}{\sim} N(0, \sigma^2)$$to mean that as the sample size $N$ gets larger, $\hat{\theta}_N$ is approximately normally distributed with mean 0 and variance $\sigma^2$.
* **Central Limit Theorem**
	* Let $\{Y_1, Y_2, \ldots, Y_N\}$ be a random sample with mean $\mu$ and variance $\sigma^2$. Then,$$Z_N=\frac{\bar{Y}_N-\mu}{\sigma / \sqrt{N}} \stackrel{a}{\sim} N(0,1)$$
	* Intuitively, it means when the sample size gets larger, the distribution of the sample average is closer to a normal distribution.
	* ä¸ç®¡Yçš„åˆ†å¸ƒå¦‚ä½•ï¼Œå½“æ ·æœ¬é‡è¶³å¤Ÿå¤§ï¼Œéƒ½ä¼šè¶‹å‘äºŽæ­£æ€åˆ†å¸ƒ

### Asymptotic Normality of OLS

* Under the Gauss-Markov Assumptions MLR.1 through MLR. 5 , for each $j=0,1, \ldots, k$
* $$\begin{aligned}
& \frac{\hat{\beta}_j-\beta_j}{s d(\hat{\beta}_j)} \stackrel{a}{\sim} \operatorname{Normal}(0,1) . \\\\
& \frac{\hat{\beta}_j-\beta_j}{\operatorname{se}(\hat{\beta}_j)} \stackrel{a}{\sim} \operatorname{Normal}(0,1) .
\end{aligned}$$
* OLS estimators are approximately normally distributed in large enough sample sizes.
* è¿™ä¸ªå®šç†è¯´æ˜Žå½“æ ·æœ¬é‡è¶³å¤Ÿå¤§çš„æ—¶å€™ï¼Œä¸éœ€è¦uçš„æ­£æ€åˆ†å¸ƒå‡å®šã€‚

### Summary

* Under MLR.1-MLR.4, OLS estimators are consistent. 
* Under MLR.1-MLR.5, OLS estimators have an asymptotic normal distribution.


## Ch6 Multiple Regression Analysis: Further Issues

### Efects of Data Scaling on OLS Statistics

#### changing unit of measurement

* Consider the simple regression model:$$y=\beta_0+\beta_1 x+u$$
* Now suppose $y^*=w_1 y$ and $x^*=w_2 x$, Then for this model:$$y^*=\beta_0^*+\beta_1^* x^*+u$$
* $\hat{\beta}_0^*$ and $\hat{\beta}_0$, and $\hat{\beta}_1^*$ and $\hat{\beta}_1$ çš„å…³ç³»æ˜¯ï¼Ÿ
	* $$\hat{\beta}_0^*=w_1 \hat{\beta}_0, \quad \hat{\beta}_1^*=\frac{w_1}{w_2} \hat{\beta}_1$$
	* $$\operatorname{se}(\hat{\beta}_0^*) =w_1 \operatorname{se}(\hat{\beta}_0), \quad\operatorname{se}(\hat{\beta}_1^*) =\frac{w_1}{w_2}\operatorname{se}(\hat{\beta}_1)$$
	* $$t_\hat{\beta_0}^*=t_\hat{\beta_0} \quad t_\hat{\beta_1}^* =t_\hat{\beta_1}$$
	* $$R^{2*}=R^2$$
	* the statistical signiicance does not change.

#### Unit Change in Logarithmic Form

*  åªå½±å“æˆªè·ï¼Œä¸å½±å“ç³»æ•°

#### Beta Coefficients
* Sometimes, it's useful to obtain regression results when all variables are standardized: subtracting off its mean and dividing by its standard deviation.
* $$
\begin{aligned}
y_i & =\hat{\beta_0}+\hat{\beta_1} x_{i 1}+\cdots+\hat{\beta_k} x_{i k}+\hat{u_i} . \\\\
(y_i-\bar{y}) / \hat{\sigma_y} & =(\hat{\sigma_1} / \hat{\sigma_y}) \hat{\beta_1}[(x_{i 1}-\bar{x_1}) / \hat{\sigma_1}]+\cdots \\\\
& +(\hat{\sigma_k} / \hat{\sigma_y}) \hat{\beta_k}[(x_{i k}-\bar{x_k}) / \hat{\sigma_k}]+(\hat{u_i} / \hat{\sigma_y}) \\\\
z_y & =\hat{b_1} z_1+\cdots+\hat{b_k} z_k+\text { error }
\end{aligned}
$$
* where $z_y$ denotes the z-score of $y$. The new coefficients are$$\hat{b}_j=(\hat{\sigma}_j / \hat{\sigma}_y) \hat{\beta}_j$$
* If $x_1$ increases by one standard deviation, then $\hat{y}$ changes by $\hat{b}_1$ standard deviation.
* å½’ä¸€åŒ–äº†


### More on Functional Form

* å¯¹äºŽlogå½¢å¼ $$z=log(y)=\beta_0+\beta_1 x+u$$
	* å½“xå˜åŒ–$\Delta$ æ—¶ï¼Œ$$\\\%\Delta E(y|x)=100[exp(\beta_1\Delta)-1]$$
	* å½“ $\Delta$ è¶‹è¿‘0æ—¶ï¼Œ$$\\\%\Delta E(y|x)\approx 100\beta_1\Delta$$
* å¯¹äºŽxå–å€¼å°äºŽ0çš„ï¼Œå¯ä»¥ä½¿ç”¨ inverse hyperbolic sine: $$IRS(x)=arcsinh(x)=log(x+\sqrt{x^2+1})$$
* å½“xå¯¹yçš„å½±å“æ˜¯éžçº¿æ€§çš„ï¼Œå¯ä»¥è€ƒè™‘äºŒæ¬¡æ–¹ã€‚

### More on Goodness of Fit

* åœ¨Ch4ä¸­é€šè¿‡Fæ£€éªŒæ¥åˆ¤æ–­æ˜¯å¦å¯ä»¥restrictæ¨¡åž‹ï¼Œé‚£ä¹ˆå¯¹äºŽnon-nested modelæ€Žä¹ˆåŠžå‘¢ï¼Ÿ
* ä¾‹å¦‚ï¼š $$\begin{aligned}
& y=\beta_0+\beta_1 x_1+\beta_2 x_2+u \\\\
& y=\gamma_0+\gamma_1 x_4+e
\end{aligned}$$
* è¿™æ—¶å€™éœ€è¦ç”¨ Adjusted R-square
	* é€‰æ‹© $\bar{R}^2$ æœ€é«˜çš„

### Prediction Analysis

#### confidence interval for E(y|x)

* Suppose we have estimated the equation$$\hat{y}=\hat{\beta}_0+\hat{\beta}_1 x_1+\hat{\beta}_2 x_2+\ldots+\hat{\beta}_k x_k$$Let $c_1, c_2, \ldots, c_k$ denote particular values for each of the $k$ independent variables.
* The parameter we would like to estimate is$(\theta)=E(y \mid x_1=c_1, \ldots, x_k=c_k)=\beta_0+\beta_1 c_1+\beta_2 c_2+\ldots+\beta_k c_k$
* The estimator of $\theta$ is$$\hat{\theta}=\hat{\beta}_0+\hat{\beta}_1 c_1+\hat{\beta}_2 c_2+\ldots+\hat{\beta}_k c_k$$
* $$y=\theta+\beta_1(x_1-c_1)+\beta_2(x_2-c_2)+\ldots+\beta_k(x_k-c_k)+u$$
- So we can regress $y_i$ on $(x_{i 1}-c_1), \ldots,(x_{i k}-c_k)$. The standard error and confidence interval of the intercept of this new regression is what we need.

#### Prediction Interval

* $$y=E(y \mid x_1, \ldots, x_k)+u$$
* The previous method form a confidence interval for $E(y \mid x_1, \ldots, x_k)$.
- Sometimes we are interested in forming the confidence interval for an unknown outcome on $y$.
- We need to account for the variation in $u$.


* Let $x_1^0, \ldots, x_k^0$ be the new vales of the independent variables, which we assume we observe. Let $u^0$ be the unobserved error.
$$
y^0=\beta_0+\beta_1 x_1^0+\ldots+\beta_k x_k^0+u^0 .
$$
* Our best prediction of $y^0$ is estimated from the OLS regression line
$$
\hat{y}^0=\hat{\beta}_0+\hat{\beta}_1 x_1^0+\ldots+\hat{\beta}_k x_k^0
$$
* The prediction error in using $\hat{y}^0$ to predict $y^0$ is
* Note $E(\hat{y}^0)=y^0$, because the $\hat{\beta}_j$ are unbiased. Because $u^0$ has zero mean, $E\left(\hat{e}^0\right)=0$.
* Note that $u^0$ is uncorrelated with each $\hat{\beta}_j$, because $u^0$ is uncorrelated with the errors in the sample used to obtain the $\hat{\beta}_j$.
- Therefore, the variance of the prediction error (conditional on all in-sample values of the independent variables) is:
$$
\operatorname{Var}(\hat{e}^0)=\operatorname{Var}(\hat{y}^0)+\operatorname{Var}(u^0)=\operatorname{Var}(\hat{y}^0)+\sigma^2 .
$$
* The standard error of $\hat{e}^0$ is:$$se(\hat{e}^0)=\{[se(\hat{y}^0)]^2+\hat{\sigma}^2\}^{1 / 2}$$
* The prediction interval for $y^0$ is
$$
\sqrt{\hat{y}^0 \pm c \cdot s e\left(\hat{e}^0\right)}
$$


## Ch7 Multiple Regression Analysis with Qualitative Information

### A Single Dummy Independent Variable

* We often capture binary information by defining binary variable or a zero-one variable.$$\text { female }= \begin{cases}0, & \text { if the individual is man } \\\\ 1, & \text { if the individual is woman }\end{cases}$$
* zero-one leads to natural interpretations of the regression parameters

* å‡è®¾ $$\text { wage }=\beta_0+\delta_0 \text { female }+u$$
* If we estimate the model using OLS$$\begin{aligned}
& \hat{\beta_0}=\overline{w a g e}_{m e n} \\\\
& \hat{\delta_0}=\overline{w a g e}_{women}-\overline{w a g e}_{m e n}\end{aligned}$$
* If we regress $y$ on a dummy variable $x$, then the OLS estimate of the intercept represents the sample average of $y$ when $x=0$, the OLS estimate of the slope coefficient represents the difference between the sample average of $y$ when $x=1$ and $x=0$.

* å½“å¢žåŠ å˜é‡æ—¶ï¼Œä¾‹å¦‚ $$w a g e=\beta_0+\delta_0 \text { female }+\beta_1 e d u c+u$$
* $Î´_0$ is the difference in hourly wage between women and men, given the same amount of education.

#### çº¿æ€§ç›¸å…³

* $wage=\beta_0 + \beta_1female+u$ æ˜¯å¯ä»¥çš„
	* é€‰å®šäº† male ä½œä¸º baseline group
* $wage=\beta_0 male + \beta_1female+u$ æ˜¯å¯ä»¥çš„
* $wage=\beta_0+\beta_1 male + \beta_2female+u$ æ˜¯ä¸è¡Œçš„

#### Without intercept

* $wage=\beta_0 male + \beta_1female+u$ æ²¡æœ‰æˆªè·ï¼Œåœ¨è§£é‡Šå’Œè®¡ç®—æ£€éªŒçš„æ—¶å€™ä¸æ–¹ä¾¿ï¼Œå¹¶ä¸” $R^2$ å¯èƒ½æ˜¯è´Ÿçš„ã€‚
* In general, if there is no intercept in the regression model, the $R^2$ could be negative.
* To address the issue, some researchers use the **uncentered R-squared** when there is no intercept in the model$$R_0^2=1-\frac{S S R}{S S T_0},$$where $S S T_0=\sum_{i=1}^N y_i^2$.

### Using Dummy Variables for Multiple Categories

* å¤šä¸ªç»„åˆ«çš„æ—¶å€™ï¼Œä¸¤ç§é€‰æ‹©
* ç¬¬ä¸€ç§ï¼Œk-1ä¸ªç‹¬ç«‹çš„å˜é‡ï¼š
	* $$\text { wage }=\beta_0+\beta_1 \text { marrmale }+\beta_2 \text { marrfemale }+\beta_3 \text { singfem }+u \text {. }$$
* ç¬¬äºŒç§ï¼Œä¹˜ç§¯é¡¹ï¼š
	* $$\text { wage }=\beta_0+\beta_1 \text { female }+\beta_2 \text { married }+\beta_3 \text { female } \cdot \text { married }+u$$

* æ³¨æ„å¦‚æžœä¸€ä¸ªå˜é‡åªèƒ½åœ¨å‡ ä¸ªç¦»æ•£å€¼ä¹‹é—´é€‰æ‹©ï¼Œæœ€å¥½æŠŠæ¯ä¸ªå€¼çš„æƒ…å†µç‹¬ç«‹æˆä¸€ä¸ªå“‘å…ƒï¼Œå¦åˆ™ç›¸å½“äºŽæš—ç¤ºäº†æ–œçŽ‡å˜åŒ–æ˜¯çº¿æ€§çš„ã€‚
	* ä¾‹å¦‚CRå¯ä»¥å–0ï¼Œ1ï¼Œ2ï¼Œ3ï¼Œ4
	* $$MBR=\beta_0+\beta_1 C R+\text { other factors }+u \text {. }$$
	* $$MBR=\beta_0+\delta_1 C R 1+\delta_2 C R 2+\delta_3 C R 3+\delta_4 C R 4+\text{other factors}+u$$
	* ç¬¬äºŒç§æ¨¡åž‹æ›´å¥½ã€‚

### Interactions Involving Dummy Variables

* åœ¨ $wage=\beta_0+\beta_1female+\beta_2educ+u$ ä¸­æˆ‘ä»¬å‡å®šäº† educ å¯¹äºŽç”·å¥³çš„å½±å“æ˜¯ç›¸åŒçš„ã€‚
* ä¸ºäº†åŒºåˆ«ï¼Œéœ€è¦æ·»åŠ ä¸€ä¸ªä¹˜ç§¯é¡¹ï¼š$$E(\text { wage } \mid \text { female }, \text { educ })=\beta_0+\delta_0 \text { female }+\beta_1 \text { educ }+\delta_1 \text { female } \cdot \text { educ. }$$
#### Testing for Differences in Regression Functions across Groups

* $$\text { wage }=\beta_0+\beta_1 \text { educ }+\beta_2 \text { exper }+\beta_3 \text { tenure }+u$$
- We want to test whether all the coefficients are the same for men and women.
- We can include interactive terms for all variables:$$
\begin{aligned}
\text { wage }= & \beta_0+\delta_0 \text { female }+\beta_1 \text { educ }+\delta_1 \text { educ } \cdot \text { female }+ \\\\
& \beta_2 \text { exper }+\delta_2 \text { exper } \cdot \text { female }+ \\\\
& \beta_3 \text { tenure }+\delta_3 \text { tenure } \cdot \text { female }+u .
\end{aligned}$$
* The null hypothesis:$$H_0: \delta_0=0, \delta_1=0, \delta_2=0, \delta_3=0$$We can use F test to test the hypothesis: estiamte the unrestricted and restricted model, and then calculate the F-stat.

#### Chow statistic

* å¯¹äºŽåªæœ‰ä¸€ä¸ªäºŒå…ƒå˜é‡å’Œå¾ˆå¤šå…¶ä»–è¿žç»­å˜é‡çš„å›žå½’ï¼Œæˆ‘ä»¬æƒ³åˆ¤æ–­å…¶ä»–æ‰€æœ‰çš„å˜é‡æ˜¯å¦å…³äºŽä¸¤ç»„å®Œå…¨ç›¸åŒ
* We can show that the sum of squared residuals from the unrestricted model can be obtained from two separate regressions, one for each group: $S S R_{u r}=S S R_1+S S R_2$
* The F-statstic:$$F=\frac{S S R_p-(S S R_1+S S R_2)}{S S R_1+S S R_2} \cdot \frac{N-2(k+1)}{k+1}$$$S S R_p$ : SSR from pooling the groups and estimating a single equation. 
* This is also called a Chow statistic.
* Note: use the Chow test if
	* the model satisfies homoskedasticity
	* we want to test no differences at all between the groups

### Program Evaluation

* åœ¨ç¤¾ä¼šå­¦å®žéªŒä¸­æŽ§åˆ¶å˜é‡æ³•


## Ch8 Heteroskedasticity

### Consequence of Heteroskedasticity for OLS

* Heteroskedasticity does not cause bias or inconsistency in the OLS estimators
	* å¯¹äºŽä¸€è‡´æ€§å’Œæ— åæ€§æ— å½±å“
* The interpretation of our goodness-of-fit measures is also unaffected by the presence of heteroskedasticity.
	* å¯¹äºŽgoodness of fit æ— å½±å“
	- $R^2$ and adj- $R^2$ are different ways of estimating the population R-squared, $1-\sigma_u^2 / \sigma_y^2$.
	- both variances in the population $R^2$ are unconditional variances
	- SSR/ $N$ consistently estimates $\sigma_u^2$, and $S S T / N$ consistently estimates $\sigma_y^2$, whether or $\operatorname{not} \operatorname{Var}(u \mid x)$ is constant
- With heteroskedasticity, $\operatorname{Var}\left(\hat{\beta}_j\right)$ is biased.
	- å¯¹äºŽæ–¹å·®æœ‰å½±å“
	- æ ‡å‡†å·®ï¼Œtæ•°æ®ï¼Œç½®ä¿¡åŒºé—´éƒ½ä¸å†å¯é 
	- å¤§æ ·æœ¬ä¹Ÿä¸èƒ½è§£å†³
	- OLS is no longer BLUE.

### Heteroskedasticity-Robust Inference after OLS Estimation

 * Consider the simple linear regression model:$$y_i=\beta_0+\beta_1 x_i+u_i$$Assume SLR.1-SLR.4 are satisfied, and there exists heteroskedasticity:$$\operatorname{Var}(u \mid x_i)=\sigma_i^2$$$\operatorname{Var}(u)$ takes on different values when $x$ varies
- We don't know the exact functional form of $\sigma_i^2$, it can be any function of $x$

#### Estimating $\operatorname{Var}\left(\hat{\beta}_j\right)$ under Heteroskedasticity

* One valid estimator ï¼ˆWhite,1980ï¼‰:$$\widehat{\operatorname{Var}}(\hat{\beta_1})=\frac{\sum_{i=1}^N(x_i-\bar{x})^2 \hat{u_i}^2}{[\sum_{i=1}^N(x_i-\bar{x})^2]^2} \equiv \frac{\sum_{i=1}^N(x_i-\bar{x})^2 \hat{u_i}^2}{SST_x^2}$$where $$SST_x=\sum_{i=1}^N(x_i-\bar{x})^2$$.
* For multiple regression model:$$\begin{gathered}y_i=\beta_0+\beta_1 x_{i 1}+\beta_2 x_{i 2}+\ldots+\beta_k x_{i k}+u_i . \\\\ \operatorname{Var}(\hat{\beta_j})=\frac{\sum_{i=1}^N \hat{r_{ij}}^2 \sigma_i^2}{[\sum_{i=1}^N \hat{r_{ij}}^2]^2} \equiv \frac{\sum_{i=1}^N \hat{r_{ij}}^2 \sigma_i^2}{SSR_j^2}\end{gathered}$$The estimator:$$\widehat{\operatorname{Var}}(\hat{\beta_j})=\frac{\sum_{i=1}^N \hat{r_{ij}}^2 \hat{u_i}^2}{[\sum_{i=1}^N \hat{r_{ij}}^2]^2}=\frac{\sum_{i=1}^N \hat{r_{ij}}^2 \hat{u_i}^2}{SSR_j^2}$$
	* $\hat{r_{ij}}$ is the residual from regressing $x_j$ on all other independent variables
	* $S S R_j$ is the sum of residual squared of this regression
	* The square root of $\widehat{\operatorname{Var}}\left(\hat{\beta}_j\right)$ is called the heteroskedasticity-robust standard error, or simply, robust standard errors.
* Robust-standard error æ˜¯ä¸€è‡´çš„

#### Compare the variance formula

* Under homoskedasticity, $\operatorname{Var}(\hat{\beta_1})$ is simplified as$$\operatorname{Var}(\hat{\beta_j})=\frac{\sum_{i=1}^N \hat{r_{ij}}^2 \sigma_i^2}{SSR_j^2}=\frac{\sigma^2}{SSR_j}$$
* Under heteroskedasticity,$$\operatorname{Var}(\hat{\beta_j})=\frac{\sum_{i=1}^N r_{i j}^2 \sigma_i^2}{SSR_j^2}=\frac{1}{SSR_j} \sum_{i=1}^N \frac{\hat{r_{ij}}^2}{SSR_j} \sigma_i^2=\frac{1}{SSR_j} \sum_{i=1}^N w_{ij} \sigma_i^2$$where $w_{ij}=\frac{\hat{\tau_{ij}}^2}{S S R_j}$. We know that $w_{ij}>0$ and $\sum_{i=1}^N w_{ij}=1$.
* å³ï¼Œè¿›è¡Œäº†åŠ æƒå¹³å‡ã€‚
* Robust standard errors can be either larger or smaller than the usual standard errors.

#### More on RSE

* åœ¨æœ‰äº›æƒ…å†µä¸‹ï¼Œç‰¹åˆ«æ˜¯å¼‚æ–¹å·®æ€§ä¸å¼ºçš„æƒ…å†µä¸‹ï¼Œrobust-standard-errorçš„è¡¨çŽ°ä¸å¦‚ä¼ ç»Ÿçš„standard-error
	* å°æ ·æœ¬æ—¶rseå­˜åœ¨è¯¯å·®
	* rseçš„æ ·æœ¬æ–¹å·®æ›´å¤§
* åœ¨å®žè·µä¸­é€šå¸¸åœ¨å¤§æ ·æœ¬æ—¶æŠ¥å‘Šrseï¼Œåœ¨å°æ ·æœ¬æ—¶éƒ½æŠ¥å‘Šã€‚

### Weighted Least Squares Estimation

#### Generalized Least Squares ï¼ˆGLSï¼‰

* Assume MLR.1-MLR.4 are satisfied:$$y_i=\beta_0+\beta_1 x_{i 1}+\ldots+\beta_k x_{i k}+u_i$$
* Assume that the variance of $u$ takes the following form:$$\operatorname{Var}(u \mid x_1, \ldots, x_k)=\sigma^2 h(x_1, \ldots, x_k)$$
* We write $\sigma_i^2=\sigma^2 h\left(x_{i 1}, \ldots, x_{i k}\right)=\sigma^2 h_i$.
- Consider an alternative regression model:$$\frac{y_i}{\sqrt{h_i}}=\beta_0 \frac{1}{\sqrt{h_i}}+\beta_1 \frac{x_{i 1}}{\sqrt{h_i}}+\ldots+\beta_k \frac{x_{i k}}{\sqrt{h_i}}+\frac{u_i}{\sqrt{h_i}}$$
* Let $\mathbf{x}$ denote all the explanatory variables. Conditional on $\mathbf{x}, E\left(u_i / \sqrt{h_i} \mid \mathbf{x}\right)=E\left(u_i \mid \mathbf{x}\right) / \sqrt{h_i}=0$.
* $\operatorname{Var}\left(u_i / \sqrt{h_i} \mid \mathbf{x}\right)=\sigma^2$, satisfying homoskedasticity.
* Denote the OLS estimator after the transformation as $\{\beta_j^*\}$
* We can prove that $\{\beta_j^*\}$ minimizes$$\sum_{i=1}^N(y_i-b_0-b_1 x_{i 1}-\cdots-b_k x_{i k})^2 / h_i$$
* Weighted least squares estimatorï¼ˆWLSï¼‰: 
	* the weight for each $\hat{u}_i$ is $1 / h_i$. We give less weight for observations with higher variance. Intuitively, they provide less information.
* $\{\beta_j^*\}$ is still one estimator for the original model, and have the same interpretation
* Because $\{\beta_j^*\}$ satisfies MLR.1-MLR.5, so it is BLUE under heteroskedasticity with the form $\sigma_i^2=\sigma^2 h_i$
* $\{\beta_j^*\}$ is also called generalized least squares estimators ï¼ˆGLSï¼‰

#### Feasible Generalized Least Squares ï¼ˆFGLSï¼‰
* å®žé™…ä¸­éœ€è¦ä¼°è®¡ $h_i$ 
- Assume $h_i$ takes the following form:$$\begin{aligned}\operatorname{Var}(u \mid x) & =\sigma^2 \exp \left(\delta_0+\delta_1 x_1+\ldots+\delta x_k\right) \\\\ u^2 & =\sigma^2 \exp \left(\delta_0+\delta_1 x_1+\ldots+\delta x_k\right) v,\end{aligned}$$where $v$ has a mean of one.
- We take $\exp (\cdot)$ to guarantee that $\operatorname{Var}(u)>0$
- Equivalently,
$$
\log \left(u^2\right)=\alpha+\delta_1 x_1+\ldots+\delta x_k+e .
$$
* As usual, we replace the unobserved $u$ with the OLS residuals $\hat{u}$, and estimate $\log \left(\hat{u}^2\right) \sim 1, x_1, \ldots x_k$, calculate the fitted value $\hat{g}_i$. Then $\hat{h}_i=\exp \left(\hat{g}_i\right)$.

##### Procedure

1. Run the regression of $y$ on $1, x_1, \ldots, x_k$, get the residual $\hat{u}_i$
2. Calculate $\log \left(\hat{u}_i^2\right)$
3. Estimate $\log \left(\hat{u}_i^2\right) \sim 1, x_1, \ldots x_k$, get the fitted value $\hat{g}_i$
4. Compute $\hat{h}_i=\exp \left(\hat{g}_i\right)$
5. Use $1 / \hat{h}_i$ as weights, estimate $y \sim 1, x_1, \ldots, x_k$ using WLS.

FGLS is consistent, and has smaller asymptotic variance than OLS.

#### WLS or RSE

* There is no guarantee that WLS is more efficient than OLS.
- It is alwasy advised to report robust standard errors with WLS.
* two solutions for heteroskedasticity:
	- Use OLS to estiamte the model, calculate the robust standard errors ï¼ˆor use the max of the conventional s.e. and robust s.e.ï¼‰
	- Use FGLS to estimate the model, report conventional s.e. or robust s.e.
* In practice, the first method is preferred in most cases


### Testing for Heteroskedasticity

#### Breusch-Pagan Test for Heteroskedasticity

* We want to know in model $y=\beta_0+\beta_1 x_1+. .+\beta_k x_k+u$, whether $u^2$ is correlated with $x$
* Estimate $y=\beta_0+\beta_1 x_1+. .+\beta_k x_k+u$, get the residual $\hat{u}$
* Estimate the following model and get $R_{\hat{u}^2}^2$ :$$\hat{u}_i^2=\delta_0+\delta_1 x_1+\ldots+\delta_k x_k+v$$
* We test $H_0: \delta_1=\ldots=\delta_k=0$
* Calculate the $L M$ statistic: $N \cdot R_{\hat{u}^2}^2$; or calculate the $F$ $\operatorname{statistic}\left[R_{\hat{u}^2}^2 / k\right] /\left[\left(1-R_{\hat{u}^2}^2\right) /(N-k-1)\right]$.
* Reject homoskedasticity if
	* test statistic $>$ critical value
	* $p<$ significance level

#### The White Test for Heteroskedasticity

* OLS standard errors are asymptotically valid if MLR.1-MLR.5 holds.
* It turns out that the homoskedasticity assumption can be replaced with the weaker assumption that the squared error, $u^2$, is uncorrelated with all the independent variables $\left(x_j\right)$, the squares of the independent variables $\left(x_j^2\right)$, and all the cross products $\left(x_j x_h, \forall j \neq h\right)$.
- When the model contain $k=2$ independent variables, the White test is based on an estimation of$$\hat{u}^2=\delta_0+\delta_1 x_1+\delta_2 x_2+\delta_3 x_1^2+\delta_4 x_2^2+\delta_5 x_1 x_2+v$$The White test for heteroskedasticity is the LM statistic for testing that all of the $\delta_j$ are zero, except for the intercept.
- Problem: with many independent variables, we uses many degrees of freedom. Solution: use $\hat{y}^2$ :$$\hat{u}^2=\delta_0+\delta_1 \hat{y}+\delta_2 \hat{y}^2+v$$We then use the $\mathrm{F}$ or LM statistic for the null hypothesis $H_0: \delta_0=\delta_2=0$.


## Ch12 Serial Correlation

### Serial Correlation

#### Times series data

* Time series data: observations on variables over time. 
* random sampling is often violated

#### Classical Assumptions about Time Series Data

1. The stochastic process $\{(x_{t 1}, \ldots, x_{t k}, y_t): t=1,2, \ldots, T\}$ follows the linear model:$$y_t=\beta_0+\beta_1 x_{t 1}+\ldots+\beta_k x_{t k}+u_t$$
2. No perfect collinearity.
3. Zero conditional mean.$$E(u_t \mid \mathbf{X})=0, t=1,2, \ldots, T$$
	* where $\mathbf{X}$ is the explanatory variables for all time periods.
	* $E\left(u_t \mid \mathbf{X}\right)=0$ means both $E\left(u_t \mid x_t\right)=0$ and also $E\left(u_t \mid x_s\right)=0, \forall t \neq s$.

* Unbiasedness of $\mathrm{OLS}$
	* Under assumptions TS.1, TS.2 and TS.3, the OLS estimators are unbiased and consistent.

#### Serial Correlation

* No serial correlation assumption:$$\operatorname{Cov}((x_t-\bar{x}) u_t,(x_s-\bar{x}) u_s \mid X)=0, \forall t \neq s$$Or$$E(u_s u_t \mid X)=0, \forall t \neq s$$
* For time-series data, this is often not true.

#### Auto-regressionï¼ŒAR

* Think about a simple regression model:$$y_t=\beta_0+\beta_1 x_t+u_t$$
* Assume that$$u_t=\rho u_{t-1}+e_t, t=1,2, \ldots, T$$where $|\rho|<1$, and $e_t$ are i.i.d with $E\left(e_t\right)=0$. This is called an autoregressive process of order one $(\operatorname{AR}(1))$.

##### Properties of AR

* Because $e_t$ is i.i.d, $u_t$ will be correlated with current and past $e_t$, but not future values. If the time series has been going on forever$$u_t =\rho u_{t-1}+e_t=\rho^k u_{t-k}+\rho^{k-1} e_{t-(k-1)}+\ldots+e_t =\sum_{j=0}^{\infty} \rho^j e_{t-j}$$
* $$E(u_t) =E(\sum_{j=0}^{\infty} \rho^j e_{t-j})=\sum_{j=0}^{\infty} \rho^j E(e_{t-j})=0$$
* We can show that$$\begin{aligned}
\operatorname{Var}(u_t) & =\operatorname{Var}(\sum_{j=0}^{\infty} \rho^j e_{t-j})=\sum_{j=0}^{\infty} \rho^{2 j} \operatorname{Var}(e_{t-j}) \\\\
& =\operatorname{Var}(e_t) \sum_{j=0}^{\infty} \rho^{2 j}=\frac{\operatorname{Var}(e_t)}{1-\rho^2}\end{aligned}$$
* Also$$\begin{aligned}\operatorname{Cov}(u_t, u_{t+1}) & =\operatorname{Cov}(u_t, \rho u_t+e_t)=\rho \operatorname{Var}(u_t) \\\\
\operatorname{Cov}(u_t, u_{t+j}) & =\rho^j \operatorname{Var}(u_t)
\end{aligned}$$
* Assume further that $\bar{x}=0$ and homoskedasticity, that is $\operatorname{Var}\left(u_t \mid X\right)=\operatorname{Var}\left(u_t\right)=\sigma^2$. Then
* $$
\begin{aligned}
\operatorname{Var}(\hat{\beta} \mid \mathbf{X}) & =\frac{\operatorname{Var}(\sum_{t=1}^T x_t u_t \mid \mathbf{X})}{S S T_x^2} \\\\
& =\frac{\sum_{t=1}^T x_t^2 \operatorname{Var}(u_t)+2 \sum_{t=1}^{T-1} \sum_{j=1}^{T-t} x_t x_{t+j} E(u_t u_{t+j})}{S S T_x^2} \\\\
& =\frac{\sigma^2}{S S T_x}+\frac{2 \sigma^2}{S S T_x^2} \sum_{t=1}^{T-1} \sum_{j=1}^{T-t} \rho^j x_t x_{t+j}
\end{aligned}$$

#### Consequence of ignore serial correlation

* ä»ç„¶æ˜¯æ— åã€ä¸€è‡´çš„
* ä½†ä¼ ç»Ÿçš„æ–¹å·®æœ‰é—®é¢˜äº†
* ä¼šä½Žä¼°æ–¹å·®
* å®Œå–„æ–¹æ³•ï¼š
	1. ä½¿ç”¨FGLS
	2. ä½¿ç”¨OLSï¼Œä¿®æ­£se

##### FGLS

* Assume TS.1-TS.3. Further, assume $\operatorname{Var}\left(u_t \mid X\right)=\sigma^2$.$$
\begin{aligned}
& y_t=\beta_0+\beta_1 x_t+u_t . \\\\
& u_t=\rho u_{t-1}+e_t, t=1,2, \ldots, T .
\end{aligned}$$
	* where $e_t$ is i.i.d and $E\left(e_t\right)=0$.
* Transform the regression:$$\begin{aligned}
y_t-\rho y_{t-1} & =(1-\rho) \beta_0+\beta_1(x_t-\rho x_{t-1})+e_t, t \geq 2 . \\\\
\tilde{y}_t & =(1-\rho) \beta_0+\beta_1 \tilde{x}_t+e_t, t \geq 2\end{aligned}$$
We can use FGLS to estimate:
1. Estimate the model using OLS and obtain the OLS residuals $\hat{u}_t$
2. Use OLS to estimate $\hat{u_t} \sim \hat{u_{t-1}}$ and obtain $\hat{\rho}$.
3. Calculate $\tilde{y_t}=y_t-\hat{\rho} y_{t-1}$ and $\tilde{x_t}=x_t-\hat{\rho} x_{t-1}$, then use OLS to regress $\tilde{y_t}$ on $\tilde{x_t}$.

##### Serial Correlation-Robust Inference after OLS

* äº†è§£å³å¯ï¼Œè®°ä½HACï¼ˆheteroskedasticity and auto-correlation consistentï¼‰
* We can show that$$AVar(\hat{\beta_1})=(\sum_{t=1}^T E(r_t^2))^{-2} Var(\sum_{t=1}^T r_t u_t)$$where $r_t$ is the error term in $x_{t 1}=\delta_0+\delta_2 x_{t 2}+\ldots+\delta_k x_{t k}+r_t$. We want to find and estimator for $A \operatorname{Var}(\hat{\beta}_1)$.
* Let $\hat{r}_t$ denote the residuals from regressing $x_1$ on all other independent variables, and $\hat{u}_t$ as the OLS residual from regressing $y$ on all $x$.
* Define$$\hat{\nu}=\sum_{t=1}^T \hat{a_t}^2+2 \sum_{h=1}^g[1-h /(g+1)](\sum_{t=h+1}^T \hat{a_t} \hat{a_{t-h}}),$$where $\hat{a_t}=\hat{r_t} \hat{u_t}$.
* Then$$s e(\hat{\beta_1})=[se_c(\hat{\beta_1}) / \hat{\sigma}]^2 \sqrt{\hat{\nu}}$$where $se_c(\hat{\beta_1})$ is the conventional standard error of $\hat{\beta}_1$, and $\hat{\sigma}$ is the square root of the sum of the OLS residual squared.
* We use $g$ to capture how much serial correlation we are allowing in computing the standard error.
* For annual data, choose $g=1$ or $g=2$
* Use a larger $g$ for larger sample size.
* When $g=1$,$$\hat{\nu}=\sum_{t=1}^T \hat{a_t}^2+\sum_{t=2}^T(\hat{a_t} \hat{a_{t-1}})$$
* This formula is robust to arbitrary serial correlation and arbitrary heteroskedasticity. So people sometimes call this heteroskedasticity and auto-correlation consistent, or HAC, standard errors.

### Spatial Correlation

#### Data with group structure

* group structure, ä¾‹å¦‚ä¸åŒç­çº§çš„å­¦ç”Ÿï¼Œåœ¨åŒç­ä¹‹å†…æ˜¯æœ‰ç›¸å…³æ€§çš„
* Example: class size and test score$$y_{i g}=\beta_0+\beta_1 x_g+u_{i g}$$
* Use $i$ to denote student, who are randomly assign to different class $g . y_{i g}$ is the test score of student $i$ ï¼ˆwho is in class $g$ ï¼‰, $x_g$ is the class size ï¼ˆwhich has the same value for students in the same class.ï¼‰
* Assume that $E(u \mid X)=0$
- However, observations within the same $g$ is not independent ï¼ˆstudents in the same class are exposed to the same teacher and classroom...ï¼‰$$E(u_{i g} u_{j g})=\rho_u \sigma_u^2 \neq 0$$
* We call $\rho_u$ intraclass correlation coefficient.ã€
* è¿™ç§ç›¸å…³æ€§å°±å« spatial correlation
* å­˜åœ¨è¿™ç§æƒ…å†µæ—¶ï¼Œä¸€è‡´æ€§å’Œæ— åæ€§è¿˜æ˜¯ä¿è¯çš„ï¼Œä½†æ–¹å·®å’Œæ ‡å‡†å·®æœ‰å˜åŒ–ã€‚

#### Fix spatial correlation

##### OLS and Cluster Standard Errors

* The general idea is to model correlation of error terms within a group, and assume no correlation across groups. 
* groupæ•°é‡å˜å¤šçš„æ—¶å€™æ˜¯consistentçš„
* å½“æ•°é‡å¤§äºŽ42çš„æ—¶å€™å°±å¯ä»¥è®¤ä¸ºgroupæ•°é‡å¤Ÿå¤šäº†

##### Use group mean

* Estimate$$\bar{y}_g=\beta_0+\beta_1 x_g+\bar{u}_g$$by WLS using the group size as weights.
* We can generalize the method to models with microcovariates$$y_{i g}=\beta_0+\beta_1 x_g+\beta_2 w_{i g}+u_{i g}$$
	1. Estimate$$y_{i g}=\mu_g+\beta_2 w_{i g}+\eta_{i g}$$The group effects, $\mu_g$, are coefficients on a full set of group dummies.
	2. Regress the estimated group effects on group-level variables$$\hat{\mu}_g=\beta_0+\beta_1 x_g+e_g$$In this step, we could either weight by the group size, or use no weights.


## Ch9 Proxy Variable and Measurement Error

### Endogeneity and Exogeneity
* Zero conditional mean condition:$$E(u \mid x)=0$$
	* $x_j$ is endogenous if it is correlated with $u$.
	* $x_j$ is exogenous if it is not correlated with $u$.
	* Violating the zero conditional mean condition will cause the OLS estimator to be biased and inconsistent.

### Proxy Variable

* ä»£ç†å˜é‡

#### Omitted Variable Bias

* $$\log (\text { wage })=\beta_0+\beta_1 e d u c+\beta_2 a b i l+u$$
* In this model, assume that $E(u \mid e d u c,abil)=0$
* å‡è®¾é¦–è¦ç›®çš„æ˜¯ä¼°è®¡ $\beta_1$ consistently,ä¸å…³æ³¨ $\beta_2$.
- ä½†æˆ‘ä»¬æ²¡æœ‰å…³äºŽabilçš„æ•°æ®, æ‰€ä»¥åªç”¨ educ å›žå½’ $\log ($ wage $)$ 
- There is an omitted variable bias if $\operatorname{cov}(abil,educ) \neq 0$ and $\beta_2 \neq 0$.

* One solution: use proxy variable for the omitted variable
- Proxy variable: related to the unobserved variable that we would like to control for in our analysis
	- åªéœ€è¦è¿™ä¸ªå˜é‡proxy variableä¸Žabilç›¸å…³ï¼Œå³correlatedï¼Œä¸éœ€è¦å®Œå…¨ç›¸åŒ

#### Proxy

* Formally, we have a model$$y=\beta_0+\beta_1 x_1+\beta_2 x_2^*+u$$
* Assume that $E\left(u \mid x_1, x_2^*\right)=0$
* $x_1$ is observed and $x_2^*$ is unobserved
* We have a proxy variable for $x_2^*$, which is $x_2$$$x_2^*=\delta_0+\delta_2 x_2+v_2$$
* where $v_2$ is the error to allow the possibility that $x_2$ and $x_2^*$ is not exactly related. $E\left(v_2 \mid x_2\right)=0$.
* Replace the omitted variable by the proxy variable:$$\color{red}{y=(\beta_0+\beta_2 \delta_0)+\beta_1 x_1+\beta_2 \delta_2 x_2+(u+\beta_2 v_2)}$$To get an unbiased and consistent estimator for $\beta_1$, we require$$E(u+\beta_2 v_2 \mid x_1, x_2)=0$$
* Break this down into two assumptions:
	1. $E\left(u \mid x_1, x_2\right)=0$ : the proxy variable should be exogenous ï¼ˆintuitively, since $x_2^*$ is exogenous, the proxy variable is only good if it is also exogenousï¼‰
		* ä»£ç†å˜é‡éœ€è¦æ—¶å¤–ç”Ÿçš„
	2. $E\left(v_2 \mid x_1, x_2\right)=0$ : this is equivalent as$$E(x_2^* \mid x_1, x_2)=E(x_2^* \mid x_2)=\delta_0+\delta_2 x_2$$Once $x_2$ is controlled for, the expected value of $x_2^*$ does not depend on $x_1$

* åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œå˜æˆï¼š$$\log (w a g e)=\alpha_0+\alpha_1 e d u c+\alpha_2 I Q+e$$
* In the wage equation example, the two assumptions are:
	1. $E(u \mid e d u c, I Q)=0$
	2. $E($ abil|educ, $I Q)=E(a b i l \mid I Q)=\delta_0+\delta_3 I Q$
		* The average level of ability only changes with IQ, not with education ï¼ˆonce IQ is fixedï¼‰.

* **åœ¨è¿™æ ·çš„å˜åŒ–ä¸­ï¼Œ$\beta_1$ æ˜¯æ— åçš„**
	* è¿åå‡è®¾ï¼Œä¼šé€ æˆè¯¯å·®

#### Using Lagged Dependent Variables as Proxy Variables

* æ»žåŽå› å˜é‡
* $$\text { crime }=\beta_0+\beta_1 \text { unem }+\beta_2 \text { expend }+\beta_3 \text { crime }_{-1}+u$$
* By including crime $_{-1}$ in the equation, $\beta_2$ captures the effect of expenditure of law enforcement on crime, for cities with the same previous crime rate and current unemployment rate.

### Measurement Error

#### Measurement Error in the Dependent Variable

* å› å˜é‡çš„æµ‹é‡è¯¯å·®
* Let $y^*$ denote the variable that we would like to explain.$$y^*=\beta_0+\beta_1 x_1+\ldots+\beta_k x_k+u,$$and we assume it satisfies the Gauss-Markov assumptions.
* Let $y$ to denote the observed measure of $y^*$
* Measurement error is defined as$$e_0=y-y^*$$
* Plug in and rearrange$$y=\beta_0+\beta_1 x_1+\ldots+\beta_k x_k+u+e_0$$
* å½“eå’Œè‡ªå˜é‡éƒ½æ— å…³çš„æ—¶å€™ï¼Œç»“æžœä»ç„¶æ˜¯ä¸€è‡´ä¸”æ— åçš„ï¼Œä½†æ–¹å·®ä¼šå˜å¤§
* ä»ç„¶é€‚ç”¨OLS

#### Measurement Error in the Independent Variable

* è‡ªå˜é‡çš„æµ‹é‡è¯¯å·®
* Consider a simple regression model:$$y=\beta_0+\beta_1 x_1^*+u$$We assume it satisfies the Gauss-Markov assumptions.
- We do not observe $x_1^*$. Instead, we have a measure of $x_1^*$; call it $x_1$
- The measurement error$$e_1=x_1-x_1^*$$Assume $E\left(e_1\right)=0$.
* Plug in $x_1^*=x_1-e_1$$$y=\beta_0+\beta_1 x_1+(u-\beta_1 e_1)$$
* To derive the properties of the OLS estimators, we need assumptions.
* First, assume that:$$E(u \mid x_1^*, x_1)=0$$This implies $E\left(y \mid x_1^*, x_1\right)=E\left(y \mid x_1^*\right): x_1$ does not affect $y$ after $x_1^*$ has been controlled for.
- Next, we consider two (mutually exclusive) cases about how the measurement error is correlated with $x$
1. $\operatorname{Cov}(x_1, e_1)=0$
2. $\operatorname{Cov}(x_1^*, e_1)=0$

##### Case 1ï¼š$\operatorname{Cov}(x_1, e_1)=0$

* Plug in $x_1^*=x_1-e_1$$$y=\beta_0+\beta_1 x_1+(u-\beta_1 e_1)$$
* Then $E\left(u-\beta_1 e_1 \mid x_1\right)=0$, so the OLS estimator of the slope coefficient of $x_1$ in the above model gives us unbiased and consistent estimator of $\beta_1$.
* If $u$ is uncorrelated with $e_1$, then $\operatorname{Var}\left(u-\beta_1 e_1\right)=\sigma_u^2+\beta_1^2 \sigma_{e_1}^2$.
* ä¸€è‡´æ€§å’Œæ— åæ€§ä»ç„¶æˆç«‹

##### Case 2ï¼š$\operatorname{Cov}(x_1^*, e_1)=0$

* The classical **errors-in-variables ï¼ˆCEVï¼‰** assumption is that $e_1$ is uncorrealted with the unobserved variables.
* Idea: the two components of $x_1$ is uncorrelated$$x_1=x_1^*+e_1$$
* Plug in $x_1^*=x_1-e_1$$$y=\beta_0+\beta_1 x_1+(u-\beta_1 e_1)$$
* Then$$\operatorname{Cov}(u-\beta_1 e_1, x_1)=-\beta_1 \operatorname{Cov}(x_1, e_1)=-\beta_1 \sigma_{e_1}^2 \neq 0$$
* æ­¤æ—¶ä¸€è‡´æ€§å’Œæ— åæ€§éƒ½ç ´åäº†
* The probability limit of $\hat{\beta}_1$
	* $$\operatorname{plim}(\hat{\beta}_1) =\beta_1+\frac{\operatorname{Cov}(x_1, u-\beta_1 e_1)}{\operatorname{Var}(x_1)}=\beta_1-\frac{\beta_1 \sigma_{e_1}^2}{\sigma_{x_1}^{2 *}+\sigma_{e_1}^2}=\beta_1(\frac{\sigma_{x_1}^{2 *}}{\sigma_{x_1}^{2 *}+\sigma_{e_1}^2}) $$
	* $\operatorname{plim}(\hat{\beta}_1)$ is closer to zero than $\beta_1$.
* This is called the attenuation bias in OLS due to CEV
* If the variance of $x_1^*$ is large relative to the variance in the measurement error, then the inconsistency in OLS will be small.

##### Case 2ï¼š$\operatorname{Cov}(x_1, e_1)=0 \text{ and } \operatorname{Cov}(x_1^*, e_1)=0$

* è¿™ç§æƒ…å†µä¸‹ï¼ŒOLSå‡ ä¹Žä¸€å®šä¼šé€ æˆæ— åæ€§å’Œä¸€è‡´æ€§å¤±æ•ˆã€‚


## Ch15 Instrumental Variable

### IV Estimator

#### Omitted Variable bias

* $$\log (\text { wage })=\beta_0+\beta_1 e d u c+\beta_2 a b i l+e$$
* In this model, assume that $E(e \mid e d u c, a b i l)=0$
* åªæƒ³ä¸€è‡´åœ°ä¼°è®¡ $\beta_1$ ä¸åœ¨æ„ $\beta_2$.
- å‡è®¾æ²¡æœ‰abliçš„æ•°æ®ï¼Œåªè¿›è¡Œä¸‹é¢å›žå½’$$y=\beta_0+\beta e d u c+u$$where $u=\gamma a b i l+e$.
- Note that $E(u \mid e d u c)=E\left(\beta_2 a b i l+e \mid e d u c\right)=\beta_2 E(a b i l \mid e d u c)$. If $E(a b i l)$ changes when educ changes, then the zero conditional mean assumption is not satisfied.
* $$\begin{aligned}
\hat{\beta}_{O L S} & =\frac{\sum_{i=1}^N\left(y_i-\bar{y}\right)\left(x_i-\bar{x}\right)}{\sum_{i=1}^N\left(x_i-\bar{x}\right)^2} =\beta+\frac{\sum_{i=1}^N\left(x_i-\bar{x}\right) u_i}{\sum_{i=1}^N\left(x_i-\bar{x}\right)^2} \\\\
\hat{\beta}_{O L S} & \stackrel{p l i m}{\longrightarrow} \beta+\frac{\operatorname{cov}(x, u)}{\operatorname{var}(x)} .
\end{aligned}$$
* Since $E(u \mid x) \neq 0, E\left(\hat{\beta}_{O L S}\right) \neq \beta$, OLS is not unbiased
* Since $\operatorname{cov}(x, u) \neq 0$, OLS is not consistent.
* **æ­¤æ—¶æ— åæ€§å’Œä¸€è‡´æ€§éƒ½ä¸æ»¡è¶³**

#### Instrumental Variable ï¼ˆIVï¼‰

* ç”¨zæ¥æ›¿æ¢x
* $$y=\beta_0+\beta_1 x+u$$
* ä»ç„¶å¯ä»¥ä¿è¯ $E(u)=0$,å› ä¸ºå¯ä»¥è°ƒæ•´æˆªè·é¡¹
- With $E(u \mid x) \neq 0$, we no longer have $E(x u)=0$. 
- Estimation idea: find another variable $z$, where$$\operatorname{Cov}(x, z) \neq 0 ; \quad \operatorname{Cov}(z, u)=0$$
	* $\operatorname{Cov}(z, u)$ implies$\operatorname{Cov}(z, u)=E(u z)-E(z) E(u)=E(u z)=0$
* Use $E(u z)=0$ and $E(u)=0$ to find the sample analogue, $$\begin{aligned} & E(u z)=0 \quad \frac{1}{N} \sum_{i=1}^N z_i \hat{u}_i=0 \\\\ & E(u)=0 \quad \frac{1}{N} \sum_{i=1}^N \hat{u}_i=0\end{aligned}$$and solve the equations.
* $$\begin{aligned}
\hat{\beta}_1^{I V} & =\frac{\sum_{i=1}^N\left(y_i-\bar{y}\right)\left(z_i-\bar{z}\right)}{\sum_{i=1}^N\left(x_i-\bar{x}\right)\left(z_i-\bar{z}\right)} \\\\
\hat{\beta}_0^{I V} & =\bar{y}-\hat{\beta}_1^{I V} \bar{x}
\end{aligned}$$

* æŠŠè¿™ä¸ªå«åš IV estimator
* OLSæ±‚çš„å€¼æ˜¯ä¸€ç§ç‰¹æ®Šçš„IV estimatorï¼Œå½“z=xæ—¶ã€‚

#### Assumptions on IV

* Instrument relevance: 
	* ç›¸å…³æ€§
	* $\operatorname{Cov}(x, z) \neq 0: z$ is relevant for explaining variation in $x$. 
	* $x=\pi_0+\pi_1 z+v$, è¿›è¡Œé›¶æ£€éªŒ $H_0: \pi_1=0$.
* Instrument exogeneity: 
	* å¤–ç”Ÿæ€§
	* $\operatorname{Cov}(u, z)=0$ : ä¿è¯äº†ä¸€è‡´æ€§. 
	* ä¸èƒ½ç›´æŽ¥ä»Žæ•°æ®ä¸­æ£€éªŒï¼Œéœ€è¦æ ¹æ®é‡‘èžç†è®ºã€‚



### Properties and Inference with the IV Estimator

* ä¸€è‡´æ€§æ»¡è¶³
* æ— åæ€§ä¸æ»¡è¶³
	* consider the expectation of $\hat{\beta}_1^{I V}$ conditional on $z$$$E(\hat{\beta}_1^{I V})=\beta+E(\frac{\sum_{i=1}^N(z_i-\bar{z}) u_i}{\sum_{i=1}^N(x_i-\bar{x})(z_i-\bar{z})})=\beta+E(E[\frac{\sum_{i=1}^n(z_i-\bar{z}) u_i}{\sum_{i=1}^N(x_i-\bar{x})(z_i-\bar{z})} \mid z])$$
	* ç”±äºŽxä¸æ˜¯å¸¸æ•°ï¼Œå› æ­¤ä¸èƒ½è¿›ä¸€æ­¥åŒ–ç®€
* æ–¹å·®ï¼š
	* å¢žåŠ å‡è®¾$$E[u^2 \mid z]=\sigma^2$$
	* åœ¨å¦‚ä¸Šå‡è®¾æƒ…å†µä¸‹ï¼š$$A V a r(\hat{\beta}_1^{I V})=\frac{\sigma^2}{N \sigma_x^2 \rho_{x, z}^2}$$
	* å…¶ä¸­ $\sigma_x^2$ æ˜¯æ€»ä½“xçš„æ–¹å·®ï¼Œ$\sigma^2$ æ˜¯æ€»ä½“uçš„æ–¹å·®ï¼Œ $\rho_{x,z}^2$ æ˜¯æ€»ä½“xå’Œzçš„ç›¸å…³æ€§
	* The asymptotic variance of $\hat{\beta}_1^{I V}$ is $$\widehat{A V a r}(\hat{\beta}_1^{I V})=\frac{\hat{\sigma}^2}{S S T_x R_{x, z}^2}$$where $S S T_x=\sum_{i=1}^n\left(x_i-\bar{x}\right)^2$, and $R_{x, z}^2$ is the R-squared of $x_i$ on $z_i$.
	- Note that the variance of the OLS estimator is$$\widehat{\operatorname{Var}}(\hat{\beta}_1^{O L S})=\frac{\hat{\sigma}^2}{S S T_x}$$
	- **So the IV estimator has a larger variance.**
	* If $x$ an $z$ are only slightly correlated, then $R_{x, z}^2$ can be small, and this translate into a large sampling variance of the IV estimator.


### Two Stage Least Squares

#### Multiple Instrumental Variables

* æœ‰å¯èƒ½ä¸åªä¸€ä¸ªIV
* è€ƒè™‘æ¨¡åž‹$$y=\beta_0+\beta_1 x_1+\beta_2 x_2+u$$
* Assume $x_1$ is endogenous and has two IVs: $z_1$ and $z_2$. Assume $x_2$ is exogenous.
* Two-stage least squares ï¼ˆ2SLSï¼‰: ä½¿ç”¨ä¸¤ä¸ªIVçš„çº¿æ€§ç»„åˆæ¥æž„é€ æ–°çš„IV

#### 2SLS

* æŽ¥ç€ä¸Šé¢çš„ä¾‹å­
* The steps of 2SLS
	1. Stage 1: estimate ï¼ˆusing OLSï¼‰$$x_1=\alpha_0+\alpha_1 z_1+\alpha_2 z_2+\alpha_3 x_2+v$$and calculate $\hat{x}_1$.
		* è¿™ä¸ªé˜¶æ®µçš„å›žå½’éœ€è¦åŒ…å«æ‰€æœ‰çš„å¤–ç”Ÿå˜é‡
	2. Stage 2: use $\hat{x}_1$ as an IV for $x_1$. Or directly estimate$$y=\beta_0+\beta_1 \hat{x}_1+\beta_2 x_2+u$$

* å¤šä¸ªå†…ç”Ÿå˜é‡æ—¶ï¼š
	* è€ƒè™‘æœ‰ä¸¤ä¸ªå†…ç”Ÿå˜é‡çš„æ¨¡åž‹:$$y=\beta_0+\beta_1 x_1+\beta_2 x_2+\beta_3 x_3+u$$where $x_1$ and $x_2$ are endogenous ï¼ˆwhose IV are $z_1$ and $z_2$ï¼‰, $x_3$ is exogenous.

	* In the first stage, we need to include all instruments and exogenous variables on the right hand side$$\begin{aligned}
& x_1=\alpha_0+\alpha_1 z_1+\alpha_2 z_2+\alpha_3 x_3+v . \\\\
& x_2=\gamma_0+\gamma_1 z_1+\gamma_2 z_2+\gamma_3 x_3+v .\end{aligned}$$
* **IVçš„æ•°é‡åº”è¯¥å¤§äºŽç­‰äºŽå†…ç”Ÿå˜é‡çš„æ•°é‡**

### Issues with IV

#### Sample size

* éœ€è¦ä¸€ä¸ªå¤§æ ·æœ¬ï¼Œå› ä¸ºåœ¨2SLSçš„ç¬¬ä¸€æ­¥ä¸­ï¼Œ$x=\alpha_0+\alpha_1z+e$
* å…¶ä¸­çš„$\alpha_0+Î±_1z$ æ˜¯ä¸Žuæ— å…³çš„ï¼Œeæ˜¯ä¸Žuç›¸å…³çš„
* åœ¨2SLSç¬¬äºŒæ­¥ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›ç”¨$\alpha_0+\alpha_1 z$ æ¥è¡¨ç¤ºx,ä½†å®žé™…ä¸Šæ˜¯ç”¨ $\hat{\alpha}_0+\hat{\alpha}_1 z$. åŽé¡¹å¯èƒ½åŒ…å«å…³äºŽuçš„ä¿¡æ¯ã€‚
* å› æ­¤éœ€è¦å¤§æ ·æœ¬ã€‚

#### Weak instruments

* Weak instruments: low correlation between $x$ and $z$
* Suppose there is some small correlated between $u$ and $z$$$\begin{aligned}\operatorname{plim}(\hat{\beta}_1^{I V}) & =\beta_1+\frac{\operatorname{Cov}(z, u)}{\operatorname{Cov}(z, x)} \\\\ & =\beta_1+\frac{\operatorname{Corr}(z, u)}{\operatorname{Corr}(z, x)} \cdot \frac{\sigma_u}{\sigma_x},\end{aligned}$$where $\sigma_u$ and $\sigma_x$ are the standard deviations of $u$ and $x$ in the population respectively.
* We can show that$$\operatorname{plim}(\hat{\beta}_1^{O L S})=\beta_1+\operatorname{Corr}(x, u) \cdot \frac{\sigma_u}{\sigma_x}$$
* If $\operatorname{Corr}(z, x)$ is small enough, then even if $\operatorname{Corr}(z, u)$ is small, the IV estimator could result in larger asymptotic bias than the OLS estimator.
* å¯¹äºŽæ— åæ€§åœ¨å°æ ·æœ¬ä¸‹ä¹Ÿæœ‰å½±å“ã€‚

##### æ£€éªŒå¼±ç›¸å…³

* $$y=\beta_0+\beta_1 x_1+\beta_2 x_2+u$$
* Assume $x_1$ is endogenous with two instrumental variables $z_1$ and $z_2$, and $x_2$ is exogenous.
* Estimate$$x_1=\alpha_0+\alpha_1 z_1+\alpha_2 z_2+\alpha_3 x_2+e$$
* Test $H_0: \alpha_1=\alpha_2=0$
* å½“F-statå¤§äºŽ10çš„æ—¶å€™å¯ä»¥è¯´æ²¡æœ‰å¼±ç›¸å…³æ€§

#### Testing for endogeneity

* æ£€éªŒxä¸Žuæ˜¯å¦ç›¸å…³
* Suppose $x_1$ is endogenous, and the IV is $z$$$y=\beta_0+\beta_1 x_1+\beta_2 x_2+u$$
* First stage: $x_1=\alpha_0+\alpha_1 z+\alpha_2 x_2+v$. If $x_1$ is correlated with $u$, then it must be $v$ is correlated with $u$. Estimate the equation to get $\hat{v}$.
- Estimate $y=\delta_0+\delta_1 x_1+\delta_2 x_2+\delta_3 \hat{v}+e$. Test $H_0: \delta_3=0$.

#### Testing overidentifying restrictions

* å½“zæ¯”xå¤šçš„æ—¶å€™ï¼Œå¯ä»¥æ¯”è¾ƒå¥½åœ°æŽ¨æ–­ $Cov(z,u)=0$
* Suppose $x_1$ is endogenous, and the IVs are $z_1$ and $z_2$.
* Use either one of them, we calculate $\hat{\beta}_1^{I V 1}$ and $\hat{\beta}_1^{I V 2}$
* If $\hat{\beta}_1^{I V 1}$ is very different from $\hat{\beta}_1^{I V 2}$, then at least one of them does not satisfy $\operatorname{Cov}(z, u)=0$.
* If they are close to each other, then it could be both satisfies $\operatorname{Cov}(z, u)=0$, or neither.

* å½“zå¾ˆå¤šçš„æ—¶å€™
	* Testing overidentifying restrictions:
		1. ä½¿ç”¨2SLSä¼°è®¡ï¼Œå¹¶å¾—åˆ°2SLSæ®‹é¡¹ $\hat{u}_1$.
		2. Regress $\hat{u}_1$ on all exogenous variables. Obtain the $R$-squared, say $R^2$.
		3. è‹¥æ‰€æœ‰IVéƒ½ä¸Ž $u_1$ æ— å…³ï¼Œåˆ™ $N \cdot R^2 \sim \chi_q^2$, where $q$ is the number of instrumental variables from outside the model minus the total number of endogenous explanatory variables. 
		4. Reject $H_0$ if $N \cdot R^2$ exceeds the critical value.


## Ch17 Limited Dependent Variable Models

### Linear Probability model

#### Limited Dependent Variable

* å› å˜é‡åªèƒ½å–ç‰¹å®šå€¼
* In the population, $y$ takes on two values: 0 and 1 . We are interested in how $x$ will affect $y$.
* Suppose $x$ and $y$ has this linear relation:$$y=\beta_0+\beta_1 x+u$$
* Suppose $E(u \mid x)=0$. Then$$E(y \mid x)=P(y=1 \mid x)=\beta_0+\beta_1 x$$$\beta_1$ represents when $x$ increases by one unit, the impact on the probability that $y=1$. In other words, $\beta_1.$ measures the marginal effect of $x$ on the probability that $y=1$
* yæ˜¯å¦æ˜¯äºŒå…ƒå˜é‡éƒ½ä¸å½±å“å¯¹è¿™ä¸ªæ¨¡åž‹çš„è§£é‡Š
	* Descriptiveï¼š $\beta_1$ is the expected difference in the probability that $y=1$ if $x$ changes by one unit.
	* Causal: one unit increase in $x$ causes the probability of $y=1$ to change by $\beta_1$ on average.
* è¿™ä¸ªæ¨¡åž‹è¿åäº†åŒæ–¹å·®æ€§ï¼Œå› ä¸ºæ–¹å·®æ˜¯xçš„å‡½æ•°ï¼Œå¯ä»¥ç”¨FGLSæ¥ä¼°ç®—ã€‚

### Non-linear Model and Maximum Likelihood

* Consider the following non-linear model:$$E(y \mid x)=P(y=1 \mid x)=G(\beta_0+\beta_1 x)$$where $G$ is a function mapping values to the range of 0 and 1 , to make sure $E(y \mid x)$ belongs to 0 and 1 .
* $G$ can have different functional forms. We consider two common ones:
	* logistic function ï¼ˆlogitï¼‰
		* $$G(z)=\frac{\exp (z)}{1+\exp (z)}$$
	* standard normal CDF ï¼ˆprobitï¼‰
		* $$G(z)=\Phi(z)$$
	* ![image.png](https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230409160715.png)

#### Properties of Logit and Probit

* yçš„è¿™ä¸¤ç§åˆ†å¸ƒå–å†³äºŽæ®‹é¡¹eçš„åˆ†å¸ƒ
* Suppose random variable $e$ has a CDF:$$\operatorname{Pr}(e \leq z)=G(z)$$Here $G(z)$ can be either logit or probit.
* Let $y^*=\beta_0+\beta_1 x+e$, where $e$ is independent of $x$.
* $$\begin{aligned} P(y=1 \mid x) & =P(y^*>0 \mid x) =P(\beta_0+\beta_1 x+e>0 \mid x) \\\\ & =P(e>-\beta_0-\beta_1 x) =1-\operatorname{Pr}(e \leq-\beta_0-\beta_1 x) \\\\ & =1-G\left(-\beta_0-\beta_1 x\right)=G\left(\beta_0+\beta_1 x\right) .\end{aligned}$$

#### Partial effect of x on y

- the marginal effect of $x$ on the probability that $y=1$$$\frac{\partial p(x)}{\partial x_1}=g(\beta_0+\beta_1 x) \beta_1$$where $p(x)=P(y=1 \mid x), g(z) \equiv \frac{d G}{d z}(z)$.
	- å³å¯¹ä¸Šé¢çš„å¼å­æ±‚å¯¼
- When we have more than one independent variables:$$\frac{\partial p(x)}{\partial x_j}=g(\beta_0+\boldsymbol{x} \boldsymbol{\beta}) \beta_j$$where $\boldsymbol{x} \boldsymbol{\beta}=\beta_1 x_1+\ldots+\beta_k x_k$.
* So the ratio of the partial effect of $x_j$ and $x_k$ is $\frac{\beta_j}{\beta_k}$.
* è¿™ç§æ–¹æ³•å’ŒOLSæ¯”çš„å¼±ç‚¹åœ¨äºŽè¿™ä¸ªåå¯¼æ•°çš„å‰é¡¹gå–å†³äºŽx
* è§£å†³åŠžæ³•ï¼š
	1. æ‰¾ç‰¹æ®Šç‚¹ï¼Œä¾‹å¦‚å‡å€¼ç‚¹ã€‚
		* Partial effect at the average:$$g(\hat{\beta}_0+\overline{\boldsymbol{x}}\hat{\beta}_{\mathbf{1}})=g(\hat{\beta}_0+\hat{\beta}_1 \bar{x}_1+\hat{\beta}_2 \bar{x}_2+\cdot+\hat{\beta}_k \bar{x}_k)$$
	2. æ±‚åå¯¼çš„å‡å€¼
		* Average marginal effect:$$[N^{-1} \sum_{i=1}^N g(\hat{\beta}_0+\boldsymbol{x}_{\boldsymbol{i}} \hat{\boldsymbol{\beta}}_{\mathbf{1}})] \hat{\beta}_j$$

#### Maximum Likelihood Estimation

* ä¼°è®¡ï¼šåœ¨æ ·æœ¬ä¸­è§‚å¯Ÿåˆ°æŸäº›å€¼æ—¶y=1ï¼Œå¦å¤–ä¸€äº›å€¼æ—¶y=0
* ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ‰¾åˆ° $\beta$ ä½¿å¾—è¿™ç§æˆç«‹çš„æ¦‚çŽ‡æœ€å¤§ã€‚
* ä¸‹é¢å°±æ˜¯ä¸€ä¸ªæ™®é€šçš„æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„è¿‡ç¨‹ï¼Œå½“ä½œæ¦‚ç»Ÿå¤ä¹ ï¼š

* Suppose we have a random sample of size $N$. Fix every $x$ and $\beta$, the probability that $y=1$ is:$$E(y \mid x)=P(y=1 \mid x)=G(\beta_0+\beta_1 x) \equiv G(\beta x) $$
* Then for any observation $y=0$ or $y=1$, its probability density function is:$$f(y \mid \beta x)=G(\beta x)^y[1-G(\beta x)]^{(1-y)}$$
* For a random sample, all observations are independent of each other. Then the probability that we observe the sample is: ï¼ˆ $i$ is the index for each observationï¼‰$$f(\{y_1, \ldots, y_N\} \mid \beta x_i)=\prod_{i=1}^N[G(\beta x_i)]^{y_i}[1-G(\beta x_i)]^{(1-y_i)}$$
- Maximum likelihood estimation (MLE): maximize the probability that we observe the data:$$\max _{\boldsymbol{\beta}} f(\{y_1, \ldots, y_N\} \mid \beta x_i)=\max _{\boldsymbol{\beta}} \prod_{i=1}^N[G(\beta x_i)]^{y_i}[1-G(\beta \boldsymbol{x}_{\boldsymbol{i}})]^{(1-y_i)}$$
* Take the natural logarithm and define:$$\ell_i(\beta)=\log ([G(\beta x_i)]^{y_i}[1-G(\beta x_i)]^{(1-y_i)})=y_i \log [G( x_i)]+(1-y_i) \log [1-G(\beta x_i)]$$
* Then we can equivalently write:$$\max _\beta \sum_{i=1}^N \ell_i(\beta)=\max _{\boldsymbol{\beta}} \sum_{y_i=1} \log [G(\beta x_{\boldsymbol{i}})]+\sum_{y_i=0} \log [1-G(\beta x_{\boldsymbol{i}})]$$
* **MLE is consistent and asymptotically efficient.**

#### MLE and OLS

* OLSç”¨æ¥ä¼°ç®—çº¿æ€§æ¨¡åž‹
* MLEç”¨æ¥ä¼°è®¡çº¿æ€§å’Œéžçº¿æ€§æ¨¡åž‹
* å½“uæœä»Žæ­£æ€åˆ†å¸ƒæ—¶ï¼ŒMLEä¸ŽOLSç»“æžœç›¸åŒ


## Appendix

* $\color{red}{\text{Law of Iterated Expectation:}}$
	* $$\color{red}{E(y)=E[E(y|x)]}$$
* Summation operation
	* $$\sum_{i=1}^N(x_i-\bar{x})(y_i-\bar{y})=\sum_{i=1}^N(x_i-\bar{x})y_i=\sum_{i=1}^N(x_iy_i-\bar{x}\bar{y})$$

* Variance
	* $$\begin{aligned}
\operatorname{Var}(X+a) & =\operatorname{Var}(X) \\\\
\operatorname{Var}(a X) & =a^2 \operatorname{Var}(X) \\\\
\operatorname{Var}(X) & =\operatorname{Cov}(X, X) \\\\
\operatorname{Var}(a X+b Y) & =a^2 \operatorname{Var}(X)+b^2 \operatorname{Var}(Y)+2 a b \operatorname{Cov}(X, Y) \\\\
\operatorname{Var}\left(\sum_{i=1}^N a_i X_i\right) & =\sum_{i, j=1}^N a_i a_j \operatorname{Cov}(X_i, X_j) \\\\
& =\sum_{i=1}^N a_i^2 \operatorname{Var}(X_i)+\sum_{i \neq j} a_i a_j \operatorname{Cov}(X_i, X_j) \\\\
& =\sum_{i=1}^N a_i^2 \operatorname{Var}(X_i)+2 \sum_{i=1}^N \sum_{j=i+1}^N a_i a_j \operatorname{Cov}(X_i, X_j) .
\end{aligned}$$


